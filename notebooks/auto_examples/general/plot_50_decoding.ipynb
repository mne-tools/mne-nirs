{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Decoding Analysis\n\nThis is an example of a decoding analysis performed on\nfunctional near-infrared spectroscopy (fNIRS) data using\nMNE-Python, scikit-learn, and MNE-NIRS.\n\nDetailed information about decoding of neural signals can be found\nin the MNE-Python documentation. For example see\n`Decoding (MVPA) <https://mne.tools/stable/auto_examples/decoding/decoding_csp_eeg.html>`_,\n`Linear classifier on sensor data  <mne:ex-linear-patterns>`,\n`Decoding source space data <mne:tut-dec-st-source>`.\nThis example will use the techniques covered in the MNE-Python tutorials,\nbut applied specifically to fNIRS data.\n\nThis script is an example of analysis performed in the manuscript\nLuke et. al. (2021)\n:footcite:`Luke2021.11.19.469225`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial uses data in the BIDS format.\n   The BIDS specification for NIRS data is still under development. See:\n   `fNIRS BIDS proposal <https://github.com/bids-standard/bids-specification/pull/802>`_.\n   As such, to run this tutorial you must use the fNIRS development\n   branch of MNE-BIDS.\n\n   To install the fNIRS development branch of MNE-BIDS run:\n   `pip install -U https://codeload.github.com/rob-luke/mne-bids/zip/nirs`.\n\n   MNE-Python. allows you to process fNIRS data that is not in BIDS format too.\n   Simply modify the ``read_raw_`` function to match your data type.\n   See `data importing tutorial <tut-importing-fnirs-data>` to learn how\n   to use your data with MNE-Python.</p></div>\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Robert Luke <mail@robertluke.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD (3-clause)\n\n\n# Import common libraries\nimport os\nimport contextlib\nimport numpy as np\n\n# Import sklearn processing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n# Import MNE-Python processing\nfrom mne.preprocessing.nirs import optical_density, beer_lambert_law\nfrom mne import Epochs, events_from_annotations\nfrom mne.decoding import (Scaler,\n                          cross_val_multiscore,\n                          Vectorizer)\n\n# Import MNE-NIRS processing\nfrom mne_nirs.datasets.audio_or_visual_speech import data_path\n\n# Import MNE-BIDS processing\nfrom mne_bids import BIDSPath, read_raw_bids, get_entity_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up directories\n.. sidebar:: Requires MNE-BIDS fNIRS branch\n\n   This section of code requires the MNE-BIDS fNIRS branch.\n   See instructions at the top of the page on how to install.\n   Alternatively, if your data is not in BIDS format,\n   skip to the next section.\n\nFirst we will define where the raw data is stored. We will analyse a\nBIDS dataset. This ensures we have all the metadata we require\nwithout manually specifying the trigger names etc.\nWe first define where the root directory of our dataset is.\nIn this example we use the example dataset ``audio_or_visual_speech``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = data_path()\ndataset = BIDSPath(root=root, suffix=\"nirs\", extension=\".snirf\", session=\"01\",\n                   task=\"AudioVisualBroadVsRestricted\", datatype=\"nirs\")\nsubjects = get_entity_vals(root, 'subject')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define individual analysis\n\nMore details on the epoching analysis can be found\nat `Waveform individual analysis <tut-fnirs-processing>`.\nA minimal processing pipeline is demonstrated here, as the focus\nof this tutorial is to demonstrate the decoding pipeline.\nIn this example only the epochs for the two conditions we wish to decode\nbetween are retained.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def epoch_preprocessing(bids_path):\n\n    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n        raw_intensity = read_raw_bids(bids_path=bids_path).load_data()\n\n    raw_od = optical_density(raw_intensity)\n    raw_od.resample(1.5)\n    raw_haemo = beer_lambert_law(raw_od, ppf=6)\n    raw_haemo = raw_haemo.filter(None, 0.6, h_trans_bandwidth=0.05,\n                                 l_trans_bandwidth=0.01, verbose=False)\n\n    events, event_dict = events_from_annotations(raw_haemo, verbose=False)\n    epochs = Epochs(raw_haemo, events, event_id=event_dict, tmin=-5, tmax=30,\n                    reject=dict(hbo=100e-6), reject_by_annotation=True,\n                    proj=True, baseline=(None, 0), detrend=1,\n                    preload=True, verbose=False)\n\n    epochs = epochs[[\"Control\", \"Audio\"]]\n    return raw_haemo, epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run analysis on all participants\n\nNext we loop through each measurement and decode between the control and\naudio condition.\nHere we compute a single spatio-temporal metric approach that simultaneously\nuses all channels and time points to estimate the experimental condition.\nThe data is scaled for each channel by the mean and standard deviation\nfrom all time points and epochs, after which they were vectorized to\ncomply with the scikit-learn data structure, and a logistic regression\nclassifier was applied using the liblinear solver.\nThis approach classifies the data within, rather than across, subjects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for chroma in ['hbo', 'hbr']:\n\n    st_scores = []\n    for sub in subjects:\n\n        bids_path = dataset.update(subject=sub)\n        raw_haemo, epochs = epoch_preprocessing(bids_path)\n\n        epochs.pick(chroma)\n\n        X = epochs.get_data()\n        y = epochs.events[:, 2]\n\n        clf = make_pipeline(Scaler(epochs.info),\n                            Vectorizer(),\n                            LogisticRegression(solver='liblinear'))\n\n        scores = 100 * cross_val_multiscore(clf, X, y,\n                                            cv=5, n_jobs=1, scoring='roc_auc')\n\n        st_scores.append(np.mean(scores, axis=0))\n\n    print(f\"Average spatio-temporal ROC-AUC performance ({chroma}) = \"\n          f\"{np.round(np.mean(st_scores))} % ({np.round(np.std(st_scores))})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nData were epoched then decoding was performed on the hbo signal and the hbr\nsignal. The HbO signal decodes the conditions with 6% greater accuracy\nthan the HbR signal. For further discussion about the efficacy of fNIRS\nsignals in decoding experimental condition see Luke et. al. (2021)\n:footcite:`Luke2021.11.19.469225`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bibliography\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}