{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Finger Tapping Analysis with Kernel Flow2 Data\n\nThis example follows closely the `tut-fnirs-hrf`\n`mne-nirs` finger tapping example with some minor variations,\nusing fNIRS data from the Kernel Flow 2 (KF2) system.\n\nKF2 is a time-domain (TD)-fNIRS system, and so some aspects of the\n``snirf`` file i/o are different to other mne-nirs examples which all\nuse continuous-wave data. This example thus serves as a (minimal) demo\nand test of ``mne-nirs`` for TD data, and also as an example of fNIRS\nbrain activity measurements from a high-density (~5000 channel)\nwhole-head montage.\n\nThe [dataset](https://osf.io/4nsrv) was collected at the Centre for\nAddiction and Mental Health (CAMH) in Toronto in August 2025. It consists\nof two 13-minute runs of the Kernel Finger tapping task, which is part of\nthe standard task battery distributed with the Flow2 system. Additional\nrecordings with the same task are also available on the Kernel Website.\nThe `experiment design <tut-fnirs-glm-components>` follows the usual\nstructure for motor tasks of this kind: three conditions (left-handed tapping,\nright-handed tapping, and no tapping + fixation cross), alternating\npseudo-randomly. For the tapping conditinos, a minimal hand diagram is\ndisplayed that shows red flashes on the fingertips, indicating which finger\nshould be tapped on the thumb. The highlighted finger alternates every few\nseconds, with each finger change defining a trial. Here we do not make use\nof the full event-related component of the design, but do block-wise comparisons\nbetween the three conditions.\n\nAs with the main ``mne-nirs`` finger tapping example, the following demonstrates\nan \u2018Evoked\u2019 (trial-averaging) and GLM-based analysis of this experiment.\nThere are some modifications made to the visualization code to accomodate the\n(substantially) higher channel density, and also to demonstrate an alternative\n(slightly cleaner) way of displaying symmetric contrasts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 10\n\n# Authors: Julien DuBois     <https://github.com/julien-dubois-k>\n#          John D Griffiths  <john.griffiths@utoronto.ca>\n#          Eric Larson       <https://larsoner.com>\n#\n# License: BSD (3-clause)\n\n# Importage\nimport os\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import colors as mcolors\nfrom matplotlib import pyplot as plt\nfrom mne import Epochs\nfrom mne import annotations_from_events as get_annotations_from_events\nfrom mne import events_from_annotations as get_events_from_annotations\nfrom mne.channels import combine_channels, rename_channels\nfrom mne.io.snirf import read_raw_snirf\nfrom mne.viz import plot_compare_evokeds, plot_events, plot_topomap\nfrom nilearn.plotting import plot_design_matrix\n\nfrom mne_nirs.datasets import camh_kf_fnirs_fingertapping\nfrom mne_nirs.experimental_design import create_boxcar, make_first_level_design_matrix\nfrom mne_nirs.statistics import run_glm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import raw NIRS data\n\nFirst we import the motor tapping data, These data are similar to those\ndescribed and used in the  ``MNE fNIRS tutorial <mne:tut-fnirs-processing>``\n\nAfter reading the data we resample down to 1Hz to meet github memory constraints.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# first download the data\nsnirf_dir = camh_kf_fnirs_fingertapping.data_path()\nsnirf_file = os.path.join(\n    snirf_dir,\n    \"sub-01\",\n    \"ses-01\",\n    \"nirs\",\n    \"sub-01_ses-01_task-fingertapping_nirs_HB_MOMENTS.snirf\",\n)\n\n# now load into an MNE object\nraw = read_raw_snirf(snirf_file).load_data().resample(1)\nsphere = (0.0, -0.02, 0.006, 0.1)  # approximate for the montage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get more info from the snirf file\n\nUnfortunately, a lot of useful information that is in the SNIRF file is not\nyet read by the MNE SNIRF reader. For example, the actual source and detector\nnames (which reflect the modules they belong to).\n\nFortunately, it's quite easy to find what you need in the SNIRF hdf archive from the\n[SNIRF specification](https://github.com/fNIRS/snirf/blob/master/snirf_specification.md)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "probe_keys = [\n    (\"detectorLabels\", str),\n    (\"sourceLabels\", str),\n    (\"sourcePos3D\", float),\n    (\"detectorPos3D\", float),\n]\nwith h5py.File(snirf_file, \"r\") as file:\n    probe_data = {\n        key: np.array(file[\"nirs\"][\"probe\"][key]).astype(dtype)\n        for key, dtype in probe_keys\n    }\nprint([*probe_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need data about the events.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.annotations.to_data_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately MNE didn't load the block types so we don't know whether\na block is LEFT or RIGHT tapping. Fear not! the SNIRF file has it all,\nalbeit in a convoluted format. Let's reconstruct the information here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with h5py.File(snirf_file, \"r\") as file:\n    ctr = 1\n    while (stim := f\"stim{ctr}\") in file[\"nirs\"]:\n        print(stim, np.array(file[\"nirs\"][stim][\"name\"]))\n        ctr += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like \"stim1\" has the StartBlock event information, let's dig in:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with h5py.File(snirf_file, \"r\") as file:\n    df_start_block = pd.DataFrame(\n        data=np.array(file[\"nirs\"][\"stim1\"][\"data\"]),\n        columns=[col.decode(\"UTF-8\") for col in file[\"nirs\"][\"stim1\"][\"dataLabels\"]],\n    )\ndf_start_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, `BlockType.Left` and `BlockType.Right` look useful.\nAlright, now we can make events from the MNE annotations and sort\nthem into two types, left and right tapping blocks.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the events\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events, _ = get_events_from_annotations(raw, {\"StartBlock\": 1})\nevent_id = {\"Tapping/Left\": 1, \"Tapping/Right\": 2}\nevents[df_start_block[\"BlockType.Left\"] == 1.0, 2] = event_id[\"Tapping/Left\"]\nevents[df_start_block[\"BlockType.Right\"] == 1.0, 2] = event_id[\"Tapping/Right\"]\nevents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the events\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_events(events, event_id=event_id, sfreq=raw.info[\"sfreq\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert useful events back to annotations...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "event_desc = {v: k for k, v in event_id.items()}\nannotations_from_events = get_annotations_from_events(\n    events, raw.info[\"sfreq\"], event_desc=event_desc\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set these annotations on the raw data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.set_annotations(annotations_from_events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Epoch the data\n\nThe SNIRF Hb Moments file contains data that has been\npreprocessed quite extensively and is almost \"ready for consumption\".\nDetails of the preprocessing are outlined on the\n[Kernel Docs](https://docs.kernel.com/docs/data-export-pipelines) .\nAll that remains to be done is some filtering to focus on the\n\"neural\" band. We typically use a moving-average filter for\ndetrending and a FIR filter for low-pass filtering. With MNE, we\ncan use the available bandpass FIR filter to achieve similar effects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_filt = raw.copy().filter(0.01, 0.1, h_trans_bandwidth=0.01, l_trans_bandwidth=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A little bit of MNE syntax to epoch the data with respect to the events we just\nextracted:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tmin, tmax = -5, 45\nepochs = Epochs(\n    raw_filt,\n    events,\n    event_id=event_id,\n    tmin=tmin,\n    tmax=tmax,\n    proj=True,\n    baseline=(None, 0),\n    preload=True,\n    detrend=None,\n    verbose=True,\n)\ndel raw_filt  # save memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the evoked respones\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look again at the info loaded by MNE for each channel (source-detector pair)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs.info[\"chs\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the indices of the sources and detectors from the \"channel names\"\nand also the source and detector positions so we can access the source\ndetector distance for each channel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "idx_sources = np.array(\n    [int(ch.split(\"_\")[0][1:]) - 1 for ch in epochs.info[\"ch_names\"]]\n)\nidx_detectors = np.array(\n    [int(ch.split(\"_\")[1].split(\" \")[0][1:]) - 1 for ch in epochs.info[\"ch_names\"]]\n)\nsource_positions = np.array(probe_data[\"sourcePos3D\"])[idx_sources]\ndetector_positions = np.array(probe_data[\"detectorPos3D\"])[idx_detectors]\nsds = np.sqrt(np.sum((source_positions - detector_positions) ** 2, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make `evoked` objects for the evoked response to LEFT and RIGHT tapping,\nand for the contrast left < right, for channels with a source-detector\ndistance between 15-30mm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "idx_channels = np.flatnonzero((sds > 15) & (sds < 30))\nleft_evoked = epochs[\"Tapping/Left\"].average(picks=idx_channels)\nright_evoked = epochs[\"Tapping/Right\"].average(picks=idx_channels)\ndel epochs  # save memory\nleft_right_evoked = left_evoked.copy()\nleft_right_evoked._data = left_evoked._data - right_evoked._data\nright_left_evoked = left_evoked.copy()\nright_left_evoked._data = right_evoked._data - left_evoked._data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot the evoked data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "chromophore = \"hbo\"\ntimes = [0, 10, 20, 30, 40]\nvlim = (-5e6, 5e6)\n\nplot_kwargs = dict(\n    ch_type=chromophore,\n    vlim=vlim,\n    colorbar=False,\n)\ntm_kwargs = dict(\n    sensors=False,\n    image_interp=\"linear\",\n    extrapolate=\"local\",\n    contours=0,\n    show=False,\n    sphere=sphere,\n)\n\nfig, ax = plt.subplots(\n    figsize=(1.75 * len(times), 8),\n    nrows=4,\n    ncols=len(times),\n    sharex=True,\n    sharey=True,\n    layout=\"constrained\",\n)\nleft_evoked.plot_topomap(times, axes=ax[0], **plot_kwargs, **tm_kwargs)\nright_evoked.plot_topomap(times, axes=ax[1], **plot_kwargs, **tm_kwargs)\nleft_right_evoked.plot_topomap(times, axes=ax[2], **plot_kwargs, **tm_kwargs)\nright_left_evoked.plot_topomap(times, axes=ax[3], **plot_kwargs, **tm_kwargs)\nax[0][0].set_ylabel(\"LEFT\")\nax[1][0].set_ylabel(\"RIGHT\")\nax[2][0].set_ylabel(\"LEFT  < RIGHT\")\nax[3][0].set_ylabel(\"RIGHT > LEFT\")\nfig.suptitle(chromophore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Despite the absence of thresholding, we can discern:\n\n- LEFT tapping (first row): a nice hotspot in the right motor cortex at 10s\n- RIGHT tapping (second row): a nice hotspot in the left motor cortex at 10s\n- LEFT-RIGHT tapping (last row): hotspot in the right motor cortex, and\n  negative counterpart in the left motor cortex, at 10s\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's look at the time courses:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "idx_sources = np.array(\n    [int(ch.split(\"_\")[0][1:]) - 1 for ch in left_evoked.info[\"ch_names\"]]\n)\nis_selected_hbo = np.array([ch.endswith(\"hbo\") for ch in left_evoked.info[\"ch_names\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODULE 21 is in the left motor cortex, MODULE 20 in the right motor cortex\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Channel numbers for module 20, sensor 01 and module 21, sensor 01\")\nprint(np.flatnonzero(np.array(probe_data[\"sourceLabels\"]) == \"M020S01\"))\nprint(np.flatnonzero(np.array(probe_data[\"sourceLabels\"]) == \"M021S01\"))\nis_left_motor = is_selected_hbo & (\n    idx_sources == np.flatnonzero(np.array(probe_data[\"sourceLabels\"]) == \"M021S01\")[0]\n)\nis_right_motor = is_selected_hbo & (\n    idx_sources == np.flatnonzero(np.array(probe_data[\"sourceLabels\"]) == \"M020S01\")[0]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "take a look at these and the rest of the KF2 channel locations on a montage plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10), layout=\"constrained\")\nleft_evoked.info.get_montage().plot(axes=ax, show_names=True, sphere=sphere)\nfor text in ax.texts:\n    text.set_fontsize(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And let's highlight just a couple channels of interest\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10), layout=\"constrained\")\nleft_evoked.info.get_montage().plot(axes=ax, show_names=[\"S61\", \"S64\"], sphere=sphere)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now average all channels coming from source 20 or 21 formed with detectors\nbetween 15-30mm from the source\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "right_evoked_combined = combine_channels(\n    right_evoked,\n    {\n        \"left_motor\": np.flatnonzero(is_left_motor),\n        \"right_motor\": np.flatnonzero(is_right_motor),\n    },\n)\nleft_evoked_combined = combine_channels(\n    left_evoked,\n    {\n        \"left_motor\": np.flatnonzero(is_left_motor),\n        \"right_motor\": np.flatnonzero(is_right_motor),\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and plot the evoked time series for these channel averages\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(10, 5), ncols=2, sharey=True, layout=\"constrained\")\nplot_compare_evokeds(\n    dict(\n        left=left_evoked_combined.copy().pick_channels([\"left_motor\"]),\n        right=right_evoked_combined.copy().pick_channels([\"left_motor\"]),\n    ),\n    legend=\"upper left\",\n    axes=axes[0],\n    show=False,\n    show_sensors=False,\n)\naxes[0].set_title(\"Left motor cortex\\n\\n\")\nplot_compare_evokeds(\n    dict(\n        left=left_evoked_combined.copy().pick_channels([\"right_motor\"]),\n        right=right_evoked_combined.copy().pick_channels([\"right_motor\"]),\n    ),\n    legend=False,\n    axes=axes[1],\n    show=False,\n    show_sensors=False,\n)\naxes[1].set_title(\"Right motor cortex\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GLM Analysis\n\nGLM analysis in MNE-NIRS is powered under the hood by `Nilearn` functionality.\n\nHere we mostly followed the `tut-fnirs-hrf` tutorial\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First show how the boxcar design looks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = create_boxcar(raw, stim_dur=(stim_dur := df_start_block[\"Duration\"].mean()))\nfig, ax = plt.subplots(figsize=(8, 3), layout=\"constrained\")\nax.plot(raw.times, s)\nax.legend([\"Left\", \"Right\"], loc=\"upper right\")\nax.set_xlabel(\"Time (s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now make a design matrix, including drift regressors, and plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix = make_first_level_design_matrix(\n    raw,\n    # a \"cosine\" model is better, but for speed we'll use polynomial here\n    drift_model=\"polynomial\",\n    drift_order=1,\n    high_pass=0.01,  # Must be specified per experiment\n    hrf_model=\"glover\",\n    stim_dur=stim_dur,\n)\nfig, ax = plt.subplots(figsize=(design_matrix.shape[1] * 0.5, 6), layout=\"constrained\")\nplot_design_matrix(design_matrix, axes=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now estimate the GLM model and prepare the results for viewing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# (clear channel names because mne_nirs plot_topomap doesn't have the\n# option to hide sensor names, and we have a LOT)\nrename_channels(\n    raw.info, {ch: \"\" for ch in raw.info[\"ch_names\"]}, allow_duplicates=True\n)\nprint(\"Running GLM (can take some time)...\")\nglm_est = run_glm(raw, design_matrix, noise_model=\"auto\")\ndel raw  # save memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now compute simple contrasts: LEFT, RIGHT, LEFT>RIGHT, and RIGHT>LEFT\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_matrix = np.eye(2)\nbasic_conts = dict(\n    [\n        (column, contrast_matrix[i])\n        for i, column in enumerate(design_matrix.columns)\n        if i < 2\n    ]\n)\ncontrast_L = basic_conts[\"Tapping/Left\"]\ncontrast_R = basic_conts[\"Tapping/Right\"]\ncontrast_LvR = contrast_L - contrast_R\ncontrast_RvL = contrast_R - contrast_L\n\n# compute contrasts and put into a series of dicts for plotting\ncondict_hboLR = {\n    \"LH FT HbO\": glm_est.copy().pick(\"hbo\").compute_contrast(contrast_L),\n    \"RH FT HbO\": glm_est.copy().pick(\"hbo\").compute_contrast(contrast_R),\n}\ncondict_hboLvsR = {\n    \"LH FT > RH FT HbO\": glm_est.copy().pick(\"hbo\").compute_contrast(contrast_LvR),\n    \"RH FT > LH FT HbO\": glm_est.copy().pick(\"hbo\").compute_contrast(contrast_RvL),\n}\n\ncondict_hbrLR = {\n    \"LH FT HbR\": glm_est.copy().pick(\"hbr\").compute_contrast(contrast_L),\n    \"RH FT HbR\": glm_est.copy().pick(\"hbr\").compute_contrast(contrast_R),\n}\ncondict_hbrLvsR = {\n    \"LH FT > RH FT HbR\": glm_est.copy().pick(\"hbr\").compute_contrast(contrast_LvR),\n    \"RH FT > LH FT HbR\": glm_est.copy().pick(\"hbr\").compute_contrast(contrast_RvL),\n}\n\n# make a single-color colormap with transparent \"under\" and \"bad\" (for NaNs/masked)\ncmap = plt.get_cmap(\"Reds\").copy()\ncmap.set_under((1, 1, 1, 0))  # fully transparent for values < vmin\ncmap.set_bad((1, 1, 1, 0))  # also transparent for NaNs / masked\n\n\n# finally, mmake a convenience function for plotting the contrast data\ndef plot2glmtttopos(condict, thr_p, vlim):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 6), layout=\"constrained\")\n    for ax in axes:\n        ax.set_facecolor(\"white\")  # what shows through transparency\n    for con_it, (con_name, conest) in enumerate(condict.items()):\n        t_map = conest.data.stat()\n        p_map = conest.data.p_value()\n        t_map_masked = t_map.copy()\n        t_map_masked[p_map > thr_p] = np.ma.masked\n        chromo = str(np.unique(conest.get_channel_types())[0])\n        plot_topomap(\n            t_map_masked,\n            conest.info,\n            axes=axes[con_it],\n            vlim=vlim,\n            ch_type=chromo,\n            cmap=cmap,\n            **tm_kwargs,\n        )\n        axes[con_it].set_title(con_name, fontsize=15)\n    norm = mcolors.Normalize(vmin=vlim[0], vmax=vlim[1])\n    fig.colorbar(\n        plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n        ax=axes,\n        fraction=0.05,\n        shrink=0.5,\n        orientation=\"horizontal\",\n        label=\"Stat value\",\n    )\n    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's go through each of these computed results in turn.\nNote that unlike the `tut-fnirs-hrf` tutorial, here we're following a\nstandard approach in GLM neuroimaging analysis of only viewing the positive\nvalues of A>B and B>A contrast comparisons (equivalent to doing one-tailed rather\nthan two-tailed t-tests). This is because the negative values of A>B are the\nsame as the positive values of B>A, so viewing both the +ve and -ve sides of\nboth contrasts is redundant.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start with HbO activations relative to baseline for left-handed and right-handed\ntapping. We'll also employ another standard neuroimaging approach of viewing a result\nat various significance levels, and looking at how it 'resolves down' spatially.\nHere are topoplots of the effect sizes for L>baseline and R>baseline finger tapping,\nat three signifiance threshodling levels (p<0.01, p<0.0001, p<1e-10)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot2glmtttopos(condict_hboLR, thr_p=0.01, vlim=(0.01, 10))\nfig.suptitle(\"LR HBO p < 0.01\", fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot2glmtttopos(condict_hboLR, thr_p=0.0001, vlim=(0.01, 10))\nfig.suptitle(\"LR HBO p < 0.0001\", fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot2glmtttopos(condict_hboLR, thr_p=1e-10, vlim=(0.01, 10))\nfig.suptitle(\"LR HBO p < 1e-10\", fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few comments here. First, there are two consistent zones of activation - motor\ncortex and occipital cortex - with a core pattern that does not change with\nthresholding. The fact that the pattern is visible at high thresholding levels\nindicates this is a very strong effect.\nThe motor activation is correctly located and lateralized, so\nright-handed tapping clearly activations left motor cortex, and left-handed tapping\nactivates right motor cortex. Both of these conditions also produce a strong visual\nactivation - which is expected, because the visual stimlus (see above description)\nis more complex than the inter-block fixation cross.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we then look at the contrast that compares left-handed tapping to right-handed\ntapping directly, and vice versa, we see activation of the same motor hotspots, but\nnow a much weaker contribution from the occipital lobe. This is because the visual\ncomponent is now controlled between the two conditions being compared, and what is\nbeing isolated is the difference between right-handed and left-handed tapping, which\nshould in principle be fairly well-localized to motor cortex\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot2glmtttopos(condict_hboLvsR, thr_p=1e-2, vlim=(0.01, 1.5))\nfig.suptitle(\"LvsR HbO\", fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we look at the same comparions with the HbR signal, some of the above carries\nthrough, and some does not.\n\nFirst, the basline comparisons do not replicate the patterns seen in HbO\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot2glmtttopos(condict_hbrLR, thr_p=0.1, vlim=(0.001, 1.5))\nfig.suptitle(\"LR HbR\", fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, for the hemispheric difference contrasts, we again see the hemispheric\nselectivity of the motor response according to which hand is being tapped.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot2glmtttopos(condict_hbrLvsR, thr_p=0.1, vlim=(0.001, 1.5))\nfig.suptitle(\"LvsR HbR\", fontsize=16)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}