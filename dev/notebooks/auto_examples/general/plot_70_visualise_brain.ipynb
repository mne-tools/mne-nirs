{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Utilising Anatomical Information\n\nThis example demonstrates how you can utilise anatomical and sensor position\ninformation in your analysis pipeline. This information can be used to\nverify measurement/analysis and also improve analysis accuracy\n:footcite:`novi2020integration`.\n\nThis example demonstrates how to plot your data on a 3D brain\nand overlay the sensor locations and regions of interest.\n\nThis tutorial glosses over the processing details, see the\n`GLM tutorial <tut-fnirs-hrf>` for details on the preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 5\n\n# Authors: Robert Luke <mail@robertluke.net>\n#\n# License: BSD (3-clause)\n\nimport mne\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom mne.preprocessing.nirs import beer_lambert_law, optical_density\nfrom mne_bids import BIDSPath, get_entity_vals, read_raw_bids\n\nimport mne_nirs\nfrom mne_nirs.channels import get_long_channels, get_short_channels\nfrom mne_nirs.datasets import fnirs_motor_group\nfrom mne_nirs.experimental_design import make_first_level_design_matrix\nfrom mne_nirs.io.fold import fold_landmark_specificity\nfrom mne_nirs.statistics import run_glm, statsmodels_to_results\nfrom mne_nirs.visualisation import (\n    plot_glm_surface_projection,\n    plot_nirs_source_detector,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download example data\n\nFirst, the data required data for this tutorial is downloaded.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download example fNIRS data\n\nDownload the ``audio_or_visual_speech`` dataset and load the first measurement.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = mne_nirs.datasets.audio_or_visual_speech.data_path()\ndataset = BIDSPath(\n    root=root,\n    suffix=\"nirs\",\n    extension=\".snirf\",\n    subject=\"04\",\n    task=\"AudioVisualBroadVsRestricted\",\n    datatype=\"nirs\",\n    session=\"01\",\n)\nraw = mne.io.read_raw_snirf(dataset.fpath)\nraw.annotations.rename(\n    {\"1.0\": \"Audio\", \"2.0\": \"Video\", \"3.0\": \"Control\", \"15.0\": \"Ends\"}\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download annotation information\n\nDownload the HCP-MMP parcellation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download anatomical locations\nsubjects_dir = str(mne.datasets.sample.data_path()) + \"/subjects\"\nmne.datasets.fetch_hcp_mmp_parcellation(subjects_dir=subjects_dir, accept=True)\nlabels = mne.read_labels_from_annot(\n    \"fsaverage\", \"HCPMMP1\", \"lh\", subjects_dir=subjects_dir\n)\nlabels_combined = mne.read_labels_from_annot(\n    \"fsaverage\", \"HCPMMP1_combined\", \"lh\", subjects_dir=subjects_dir\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify placement of sensors\n\nThe first thing we can do is plot the location of the optodes and channels\nover an average brain surface to verify the data, specifically the 3D coordinates,\nhave been loaded correctly. The sources are represented as red dots,\nthe detectors are represented as black dots, the whit lines represent source-detector\npairs, and the orange dots represent channel locations.\nIn this example we can see channels over the left inferior frontal gyrus,\nauditory cortex, planum temporale, and occipital lobe.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "brain = mne.viz.Brain(\n    \"fsaverage\", subjects_dir=subjects_dir, background=\"w\", cortex=\"0.5\"\n)\nbrain.add_sensors(\n    raw.info, trans=\"fsaverage\", fnirs=[\"channels\", \"pairs\", \"sources\", \"detectors\"]\n)\nbrain.show_view(azimuth=180, elevation=80, distance=450)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Plot sensor channel numbers\nOften for publications and sanity checking, it's convenient to create an\nimage showing the channel numbers along with the (typically) 10-20 location\nin the correct locations in a 3D view. The function\n:func:`mne_nirs.visualisation.plot_3d_montage` gives us this once we\nspecify which views to use to show each channel pair:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "view_map = {\n    \"left-lat\": np.r_[np.arange(1, 27), 28],\n    \"caudal\": np.r_[27, np.arange(43, 53)],\n    \"right-lat\": np.r_[np.arange(29, 43), 44],\n}\n\nfig_montage = mne_nirs.visualisation.plot_3d_montage(\n    raw.info, view_map=view_map, subjects_dir=subjects_dir\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot sensor channels and anatomical region of interest\n\nOnce the data has been loaded we can highlight anatomical regions of interest\nto ensure that the sensors are appropriately placed to measure from\nthe relevant brain structures.\nIn this example we highlight the primary auditory cortex in blue,\nand we can see that a number of channels are placed over this structure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "brain = mne.viz.Brain(\n    \"fsaverage\", subjects_dir=subjects_dir, background=\"w\", cortex=\"0.5\"\n)\nbrain.add_sensors(\n    raw.info, trans=\"fsaverage\", fnirs=[\"channels\", \"pairs\", \"sources\", \"detectors\"]\n)\n\naud_label = [label for label in labels if label.name == \"L_A1_ROI-lh\"][0]\nbrain.add_label(aud_label, borders=False, color=\"blue\")\nbrain.show_view(azimuth=180, elevation=80, distance=450)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot channels sensitive to anatomical region of interest\n\n.. sidebar:: fOLD Toolbox\n\n   You should use the fOLD toolbox to pick your optode locations\n   when designing your experiment.\n   The tool is very intuitive and easy to use.\n   Be sure to cite the authors if you use their tool or data:\n\n   Morais, Guilherme Augusto Zimeo, Joana Bisol Balardin, and Jo\u00e3o Ricardo Sato.\n   \"fNIRS optodes\u2019 location decider (fOLD): a toolbox for probe arrangement guided by\n   brain regions-of-interest.\" Scientific reports 8.1 (2018): 1-11.\n\nRather than simply eye balling the sensor and ROIs of interest, we can\nquantify the specificity of each channel to the anatomical region of interest\nand select channels that are sufficiently sensitive for further analysis.\nIn this example we highlight the left inferior frontal gyrus (IFG) and\nuse data from the fOLD toolbox :footcite:`morais2018fnirs`.\nTo see more details about how to use the fOLD data see\n`this tutorial <tut-fnirs-group-relating>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Return specificity of each channel to the Left IFG\nspecificity = fold_landmark_specificity(raw, \"L IFG (p. Triangularis)\")\n\n# Retain only channels with specificity to left IFG of greater than 50%\nraw_IFG = raw.copy().pick(picks=np.where(specificity > 50)[0])\n\nbrain = mne.viz.Brain(\n    \"fsaverage\", subjects_dir=subjects_dir, background=\"w\", cortex=\"0.5\"\n)\nbrain.add_sensors(raw_IFG.info, trans=\"fsaverage\", fnirs=[\"channels\", \"pairs\"])\n\nifg_label = [\n    label for label in labels_combined if label.name == \"Inferior Frontal Cortex-lh\"\n][0]\nbrain.add_label(ifg_label, borders=False, color=\"green\")\n\nbrain.show_view(azimuth=140, elevation=95, distance=360)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we can retain all channels and visualise the specificity of each\nchannel by encoding the specificty in the color of the line between each source and\ndetector. In this example we see that several channels have substantial specificity to\nthe region of interest.\n\nNote: this function currently doesn't support the new MNE brain API, so does\nnot allow the same behaviour as above (adding sensors, highlighting ROIs etc).\nIt should be updated in the near future.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_nirs_source_detector(\n    specificity,\n    raw.info,\n    surfaces=\"brain\",\n    subject=\"fsaverage\",\n    subjects_dir=subjects_dir,\n    trans=\"fsaverage\",\n)\nmne.viz.set_3d_view(fig, azimuth=140, elevation=95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anatomically informed weighting in region of interest analysis\n\nAs observed above, some channels have greater specificity to the desired\nbrain region than other channels.\nThus, when doing a region of interest analysis you may wish to give extra\nweight to channels with greater sensitivity to the desired ROI.\nThis can be done by manually specifying the weights used in the region of\ninterest function call.\nThe details of the GLM analysis will not be described here, instead view the\n`fNIRS GLM tutorial <tut-fnirs-hrf>`. Instead, comments are provided\nfor the weighted region of interest function call.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Basic pipeline, simplified for example\nraw_od = optical_density(raw)\nraw_haemo = beer_lambert_law(raw_od)\nraw_haemo.resample(0.3).pick(\"hbo\")  # Speed increase for web server\nsht_chans = get_short_channels(raw_haemo)\nraw_haemo = get_long_channels(raw_haemo)\ndesign_matrix = make_first_level_design_matrix(raw_haemo, stim_dur=13.0)\ndesign_matrix[\"ShortHbO\"] = np.mean(\n    sht_chans.copy().pick(picks=\"hbo\").get_data(), axis=0\n)\nglm_est = run_glm(raw_haemo, design_matrix)\n\n# First we create a dictionary for each region of interest.\n# Here we include all channels in each ROI, as we will later be applying\n# weights based on their specificity to the brain regions of interest.\nrois = dict()\nrois[\"Audio_weighted\"] = range(len(glm_est.ch_names))\nrois[\"Visual_weighted\"] = range(len(glm_est.ch_names))\n\n# Next we compute the specificity for each channel to the auditory and visual cortex.\nspec_aud = fold_landmark_specificity(\n    raw_haemo, \"42 - Primary and Auditory Association Cortex\", atlas=\"Brodmann\"\n)\nspec_vis = fold_landmark_specificity(\n    raw_haemo, \"17 - Primary Visual Cortex (V1)\", atlas=\"Brodmann\"\n)\n\n# Next we create a dictionary to store the weights for each channel in the ROI.\n# The weights will be the specificity to the ROI.\n# The keys and length of each dictionary entry must match the ROI dictionary.\nweights = dict()\nweights[\"Audio_weighted\"] = spec_aud\nweights[\"Visual_weighted\"] = spec_vis\n\n# Finally we compute region of interest results using the weights specified above\nout = glm_est.to_dataframe_region_of_interest(\n    rois, [\"Video\", \"Control\"], weighted=weights\n)\nout[\"Significant\"] = out[\"p\"] < 0.05\nout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the table above we observe that the response to the visual condition\nis only present in the visual region of interest. You can use this\ntechnique to load any custom weighting, including weights exported from\nother software.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess fNIRS data\n\nWe can also use the 3D information to project the results on to the cortical surface.\nFirst, we process the fNIRS data. This is a duplication of the GLM tutorial\nanalysis. The details will not be described here, instead view the\n`fNIRS GLM tutorial <tut-fnirs-hrf>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def individual_analysis(bids_path, ID):\n    raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)\n    raw_intensity.annotations.delete(raw_intensity.annotations.description == \"15.0\")\n    # sanitize event names\n    raw_intensity.annotations.description[:] = [\n        d.replace(\"/\", \"_\") for d in raw_intensity.annotations.description\n    ]\n\n    # Convert signal to haemoglobin and resample\n    raw_od = optical_density(raw_intensity)\n    raw_haemo = beer_lambert_law(raw_od, ppf=0.1)\n    raw_haemo.resample(0.3)\n\n    # Cut out just the short channels for creating a GLM repressor\n    sht_chans = get_short_channels(raw_haemo)\n    raw_haemo = get_long_channels(raw_haemo)\n\n    # Create a design matrix\n    design_matrix = make_first_level_design_matrix(raw_haemo, stim_dur=5.0)\n\n    # Append short channels mean to design matrix\n    design_matrix[\"ShortHbO\"] = np.mean(\n        sht_chans.copy().pick(picks=\"hbo\").get_data(), axis=0\n    )\n    design_matrix[\"ShortHbR\"] = np.mean(\n        sht_chans.copy().pick(picks=\"hbr\").get_data(), axis=0\n    )\n\n    # Run GLM\n    glm_est = run_glm(raw_haemo, design_matrix)\n\n    # Extract channel metrics\n    cha = glm_est.to_dataframe()\n\n    # Add the participant ID to the dataframes\n    cha[\"ID\"] = ID\n\n    # Convert to uM for nicer plotting below.\n    cha[\"theta\"] = [t * 1.0e6 for t in cha[\"theta\"]]\n\n    return raw_haemo, cha\n\n\n# Get dataset details\nroot = fnirs_motor_group.data_path()\ndataset = BIDSPath(\n    root=root, task=\"tapping\", datatype=\"nirs\", suffix=\"nirs\", extension=\".snirf\"\n)\nsubjects = get_entity_vals(root, \"subject\")\n\ndf_cha = pd.DataFrame()  # To store channel level results\nfor sub in subjects:  # Loop from first to fifth subject\n    # Create path to file based on experiment info\n    bids_path = dataset.update(subject=sub)\n\n    # Analyse data and return both ROI and channel results\n    raw_haemo, channel = individual_analysis(bids_path, sub)\n\n    # Append individual results to all participants\n    df_cha = pd.concat([df_cha, channel], ignore_index=True)\n\nch_summary = df_cha.query(\"Condition in ['Tapping_Right']\")\nassert len(ch_summary)\nch_summary = ch_summary.query(\"Chroma in ['hbo']\")\nch_model = smf.mixedlm(\"theta ~ -1 + ch_name\", ch_summary, groups=ch_summary[\"ID\"]).fit(\n    method=\"nm\"\n)\nmodel_df = statsmodels_to_results(ch_model, order=raw_haemo.copy().pick(\"hbo\").ch_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot surface projection of GLM results\n\nFinally, we can project the GLM results from each channel to the nearest cortical\nsurface and overlay the sensor positions and two different regions of interest.\nIn this example we also highlight the premotor cortex and auditory association cortex\nin green and blue respectively.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot the projection and sensor locations\nbrain = plot_glm_surface_projection(\n    raw_haemo.copy().pick(\"hbo\"), model_df, colorbar=True\n)\nbrain.add_sensors(\n    raw_haemo.info,\n    trans=\"fsaverage\",\n    fnirs=[\"channels\", \"pairs\", \"sources\", \"detectors\"],\n)\n\n# mark the premotor cortex in green\naud_label = [label for label in labels_combined if label.name == \"Premotor Cortex-lh\"][\n    0\n]\nbrain.add_label(aud_label, borders=True, color=\"green\")\n\n# mark the auditory association cortex in blue\naud_label = [\n    label for label in labels_combined if label.name == \"Auditory Association Cortex-lh\"\n][0]\nbrain.add_label(aud_label, borders=True, color=\"blue\")\n\nbrain.show_view(azimuth=160, elevation=60, distance=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bibliography\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}