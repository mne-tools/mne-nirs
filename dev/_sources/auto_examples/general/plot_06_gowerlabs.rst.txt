
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/general/plot_06_gowerlabs.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_general_plot_06_gowerlabs.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_general_plot_06_gowerlabs.py:


.. _tut-gowerlabs-data:

========================
Read Gowerlabs LUMO data
========================

`LUMO <https://www.gowerlabs.co.uk/lumo>`__ is a modular, wearable, 
high-density diffuse optical tomography (HD-DOT) system produced by
`Gowerlabs <https://www.gowerlabs.co.uk>`__. This tutorial demonstrates
how to load data from LUMO, and how to utilise 3D digitisation
information collected with the HD-DOT measurement.

To analyse LUMO data using MNE-NIRS, use the `lumomat <https://github.com/Gowerlabs/lumomat>`__
package to convert the native data to the SNIRF format.

HD-DOT data is often collected with individual registration of the sensor
positions. In this tutorial we demonstrate how to load HD-DOT data from a
LUMO device, co-register the channels to a head, and visualise the resulting
channel space.

This tutorial uses the 3D graphical functionality provided by MNE-Python,
to ensure you have all the required packages installed we recommend using the
`official MNE installers. <https://mne.tools/stable/install/index.html>`__

.. GENERATED FROM PYTHON SOURCE LINES 27-40

.. code-block:: default

    # sphinx_gallery_thumbnail_number = 6

    # Authors: Robert Luke <code@robertluke.net>
    #
    # License: BSD (3-clause)

    import os.path as op
    import mne
    from mne.datasets.testing import data_path

    from mne.viz import set_3d_view









.. GENERATED FROM PYTHON SOURCE LINES 41-51

Import Gowerlabs Example File
-----------------------------
First, we must instruct the software where the file we wish to import
resides. In this example we will use a small test file that is
included in the MNE testing data set. To load your own data, replace
the path stored in the `fname` variable by
running `fname = /path/to/data.snirf`.

.. note:: The provided sample file includes only a small number of LUMO
          tiles, and thus channels.

.. GENERATED FROM PYTHON SOURCE LINES 51-56

.. code-block:: default

    import mne_nirs.io

    testing_path = data_path(download=True)
    fname = op.join(testing_path, 'SNIRF', 'GowerLabs', 'lumomat-1-1-0.snirf')








.. GENERATED FROM PYTHON SOURCE LINES 57-58

We can view the path to the data by calling the variable `fname`.

.. GENERATED FROM PYTHON SOURCE LINES 58-62

.. code-block:: default


    fname






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    '/home/circleci/mne_data/MNE-testing-data/SNIRF/GowerLabs/lumomat-1-1-0.snirf'



.. GENERATED FROM PYTHON SOURCE LINES 63-64

To load the file we call the function :func:`mne:mne.io.read_raw_snirf`.

.. GENERATED FROM PYTHON SOURCE LINES 64-67

.. code-block:: default


    raw = mne.io.read_raw_snirf(fname, preload=True)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading /home/circleci/mne_data/MNE-testing-data/SNIRF/GowerLabs/lumomat-1-1-0.snirf
    /home/circleci/project/examples/general/plot_06_gowerlabs.py:65: RuntimeWarning: Extraction of measurement date from SNIRF file failed. The date is being set to January 1st, 2000, instead of unknownunknown
      raw = mne.io.read_raw_snirf(fname, preload=True)
    Reading 0 ... 273  =      0.000 ...    27.300 secs...




.. GENERATED FROM PYTHON SOURCE LINES 68-69

And we can look at the file metadata by calling the variable `raw`.

.. GENERATED FROM PYTHON SOURCE LINES 69-72

.. code-block:: default


    raw






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="table table-hover table-striped table-sm table-responsive small">
        <tr>
            <th>Measurement date</th>
        
            <td>January 01, 2000  00:00:00 GMT</td>
        
        </tr>
        <tr>
            <th>Experimenter</th>
        
            <td>Unknown</td>
        
        </tr>
            <th>Participant</th>
        
            
        
        </tr>
        <tr>
            <th>Digitized points</th>
        
            <td>5 points</td>
        
        </tr>
        <tr>
            <th>Good channels</th>
            <td>216 fNIRS (CW amplitude)</td>
        </tr>
        <tr>
            <th>Bad channels</th>
            <td>None</td>
        </tr>
        <tr>
            <th>EOG channels</th>
            <td>Not available</td>
        </tr>
        <tr>
            <th>ECG channels</th>
            <td>Not available</td>
    
        <tr>
            <th>Sampling frequency</th>
            <td>10.00 Hz</td>
        </tr>
    
    
        <tr>
            <th>Highpass</th>
            <td>0.00 Hz</td>
        </tr>
    
    
        <tr>
            <th>Lowpass</th>
            <td>5.00 Hz</td>
        </tr>
    
    
    
        <tr>
            <th>Filenames</th>
            <td>lumomat-1-1-0.snirf</td>
        </tr>
    
        <tr>
            <th>Duration</th>
            <td>00:00:28 (HH:MM:SS)</td>
        </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 73-77

Visualise Data
--------------
Next, we visually inspect the data to get an overview of the data quality
and signal annotations.

.. GENERATED FROM PYTHON SOURCE LINES 77-80

.. code-block:: default


    raw.plot(duration=60)




.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_001.png
   :alt: Raw plot
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <mne_qt_browser._pg_figure.MNEQtBrowser object at 0x7f99c8792d30>



.. GENERATED FROM PYTHON SOURCE LINES 81-87

We observe valid data in each channel, and note that the file includes a
number of event annotations.
Annotations are a flexible tool to represent events in your experiment. 
They can also be used to annotate other useful information such as bad
segments of data, participant movements, etc. We can inspect the
annotations to ensure they match what we expect from our experiment.

.. GENERATED FROM PYTHON SOURCE LINES 88-92

.. code-block:: default


    raw.annotations






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Annotations | 9 segments: A (6), Cat (1), Dog (2)>



.. GENERATED FROM PYTHON SOURCE LINES 93-100

The implementation of annotations varies between manufacturers. Rather
than recording the onset and duration of a stimulus condition, LUMO records
discrete event markers which have a nominal one second duration. Each
marker can consist of an arbitrary character or string. In this sample, 
there were six `A` annotations, one `Cat` annotation, and two `Dog` 
annotations. We can view the specific data for each annotation by converting
the annotations to a dataframe.

.. GENERATED FROM PYTHON SOURCE LINES 100-104

.. code-block:: default


    raw.annotations.to_data_frame()







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>onset</th>
          <th>duration</th>
          <th>description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>2000-01-01 00:00:04.617</td>
          <td>1.0</td>
          <td>A</td>
        </tr>
        <tr>
          <th>1</th>
          <td>2000-01-01 00:00:06.067</td>
          <td>1.0</td>
          <td>A</td>
        </tr>
        <tr>
          <th>2</th>
          <td>2000-01-01 00:00:07.317</td>
          <td>1.0</td>
          <td>A</td>
        </tr>
        <tr>
          <th>3</th>
          <td>2000-01-01 00:00:11.901</td>
          <td>1.0</td>
          <td>Cat</td>
        </tr>
        <tr>
          <th>4</th>
          <td>2000-01-01 00:00:15.518</td>
          <td>1.0</td>
          <td>Dog</td>
        </tr>
        <tr>
          <th>5</th>
          <td>2000-01-01 00:00:17.318</td>
          <td>1.0</td>
          <td>Dog</td>
        </tr>
        <tr>
          <th>6</th>
          <td>2000-01-01 00:00:21.868</td>
          <td>1.0</td>
          <td>A</td>
        </tr>
        <tr>
          <th>7</th>
          <td>2000-01-01 00:00:23.418</td>
          <td>1.0</td>
          <td>A</td>
        </tr>
        <tr>
          <th>8</th>
          <td>2000-01-01 00:00:24.802</td>
          <td>1.0</td>
          <td>A</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 105-125

View Optode Positions in 3D Space
---------------------------------
The position of optodes in 3D space is recorded and stored in the SNIRF file.
These positions are stored in head coordinate frame,
for a detailed overview of coordinate frames and how they are handled in MNE
see :ref:`mne:tut-source-alignment`.

Within the SNIRF file, the position of each optode is stored,
along with scalp landmarks (“fiducials”).
These positions are in an arbitrary space, and must be aligned to a scan of
the participants, or a generic head.

For this data, we do not have a MRI scan of the participants head.
Instead, we will align the positions to a generic head created from
a collection of 40 MRI scans of real brains called
`fsaverage <https://mne.tools/stable/auto_tutorials/forward/10_background_freesurfer.html#fsaverage>`__.

First, lets just look at the sensors in arbitrary space.
Below we see that there are three LUMO tiles, each with three sources
and four detectors.

.. GENERATED FROM PYTHON SOURCE LINES 125-134

.. code-block:: default


    subjects_dir = op.join(mne.datasets.sample.data_path(), 'subjects')
    mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir)

    brain = mne.viz.Brain('fsaverage', subjects_dir=subjects_dir, alpha=0.0, cortex='low_contrast', background="w")
    brain.add_sensors(raw.info, trans='fsaverage', fnirs=["sources", "detectors"])
    brain.show_view(azimuth=130, elevation=80, distance=700)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_002.png
   :alt: plot 06 gowerlabs
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0 files missing from root.txt in /home/circleci/mne_data/MNE-sample-data/subjects
    0 files missing from bem.txt in /home/circleci/mne_data/MNE-sample-data/subjects/fsaverage
    Channel types:: fnirs_cw_amplitude: 216




.. GENERATED FROM PYTHON SOURCE LINES 135-144

Coregister Optodes to Template Head
-----------------------------------
The optode locations displayed above are floating in free space
and need to be aligned to our chosen head.
First, lets just look at the `fsaverage` head that we will use.

.. note:: In this tutorial we use an automated code based approach
          to coregistration. You can also use the MNE-Python
          coregistration GUI :func:`mne:mne.gui.coregistration`.

.. GENERATED FROM PYTHON SOURCE LINES 144-154

.. code-block:: default


    plot_kwargs = dict(subjects_dir=subjects_dir,
                       surfaces="brain", dig=True, eeg=[],
                       fnirs=['sources', 'detectors'], show_axes=True,
                       coord_frame='head', mri_fiducials=True)

    fig = mne.viz.plot_alignment(trans="fsaverage", subject="fsaverage", **plot_kwargs)
    set_3d_view(figure=fig, azimuth=90, elevation=0, distance=1)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_003.png
   :alt: plot 06 gowerlabs
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/circleci/project/examples/general/plot_06_gowerlabs.py:150: RuntimeWarning: Digitization points not found. Cannot plot digitization.
      fig = mne.viz.plot_alignment(trans="fsaverage", subject="fsaverage", **plot_kwargs)




.. GENERATED FROM PYTHON SOURCE LINES 155-170

This is what a head model will look like. If you have an MRI from
the participant you can use freesurfer to generate the required files.
For further details on generating freesurfer reconstructions see
:ref:`mne:tut-freesurfer-reconstruction`.

In the figure above you can see the brain in grey. You can also
see the MRI fiducial positions marked with diamonds.
The nasion fiducial is marked in green, the left and right
preauricular points (LPA and RPA) in red and blue respectively.

Next, we simultaneously plot the `fsaverage` head, and the
data we wish to align to this head. This process is called
coregistration and is described in several MNE-Python tutorials
including :ref:`mne:tut-auto-coreg`.


.. GENERATED FROM PYTHON SOURCE LINES 170-174

.. code-block:: default


    fig = mne.viz.plot_alignment(raw.info, trans="fsaverage", subject="fsaverage", **plot_kwargs)
    set_3d_view(figure=fig, azimuth=90, elevation=0, distance=1)




.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_004.png
   :alt: plot 06 gowerlabs
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Channel types:: fnirs_cw_amplitude: 216




.. GENERATED FROM PYTHON SOURCE LINES 175-181

In the figure above we can see the Gowerlabs optode positions
and the participants digitised fiducials represented by circles.
The participant digitised fiducials are a large distance from MRI fiducials
on the head. To coregister the optodes to the head we will perform
a rotation and translation of the optode frame to minimise the
distance between the fiducials.

.. GENERATED FROM PYTHON SOURCE LINES 181-189

.. code-block:: default


    coreg = mne.coreg.Coregistration(raw.info, "fsaverage", subjects_dir, fiducials="estimated")
    coreg.fit_fiducials(lpa_weight=1., nasion_weight=1., rpa_weight=1.)

    fig = mne.viz.plot_alignment(raw.info, trans=coreg.trans, subject="fsaverage", **plot_kwargs)
    set_3d_view(figure=fig, azimuth=90, elevation=0, distance=1)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_005.png
   :alt: plot 06 gowerlabs
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using high resolution head model in /home/circleci/mne_data/MNE-sample-data/subjects/fsaverage/bem/fsaverage-head-dense.fif
        Triangle neighbors and vertex normals...
    Estimating fiducials from fsaverage.
    Aligning using fiducials
    Start median distance: 117.07 mm
    End   median distance:   6.13 mm
    Channel types:: fnirs_cw_amplitude: 216




.. GENERATED FROM PYTHON SOURCE LINES 190-196

We see in the figure above that after the `fit_fiducials` method was called,
the optodes are well aligned to the brain, and the distance between diamonds (MRI)
and circles (subject) fiducials is minimised. There is still some distance
between the fiducial pairs, this is because we used a generic head rather than
an individualised MRI scan. You can also :func:`mne:mne.scale_mri` to scale
the generic MRI head.

.. GENERATED FROM PYTHON SOURCE LINES 196-202

.. code-block:: default


    brain = mne.viz.Brain('fsaverage', subjects_dir=subjects_dir, background='w', cortex='0.5', alpha=0.3)
    brain.add_sensors(raw.info, trans=coreg.trans, fnirs=['sources', 'detectors'])
    brain.show_view(azimuth=90, elevation=90, distance=500)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_006.png
   :alt: plot 06 gowerlabs
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_006.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Channel types:: fnirs_cw_amplitude: 216




.. GENERATED FROM PYTHON SOURCE LINES 203-209

Apply Transformation to Raw Object
----------------------------------
You may wish to apply the coregistration transformation to the raw
object. This can be useful if you want to save the file back to disk
and not coregister again when rereading the file. Or for simpler
interface using the `fsaverage` head.

.. GENERATED FROM PYTHON SOURCE LINES 209-214

.. code-block:: default


    mtg = raw.get_montage()
    mtg.apply_trans(coreg.trans)
    raw.set_montage(mtg)






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="table table-hover table-striped table-sm table-responsive small">
        <tr>
            <th>Measurement date</th>
        
            <td>January 01, 2000  00:00:00 GMT</td>
        
        </tr>
        <tr>
            <th>Experimenter</th>
        
            <td>Unknown</td>
        
        </tr>
            <th>Participant</th>
        
            
        
        </tr>
        <tr>
            <th>Digitized points</th>
        
            <td>24 points</td>
        
        </tr>
        <tr>
            <th>Good channels</th>
            <td>216 fNIRS (CW amplitude)</td>
        </tr>
        <tr>
            <th>Bad channels</th>
            <td>None</td>
        </tr>
        <tr>
            <th>EOG channels</th>
            <td>Not available</td>
        </tr>
        <tr>
            <th>ECG channels</th>
            <td>Not available</td>
    
        <tr>
            <th>Sampling frequency</th>
            <td>10.00 Hz</td>
        </tr>
    
    
        <tr>
            <th>Highpass</th>
            <td>0.00 Hz</td>
        </tr>
    
    
        <tr>
            <th>Lowpass</th>
            <td>5.00 Hz</td>
        </tr>
    
    
    
        <tr>
            <th>Filenames</th>
            <td>lumomat-1-1-0.snirf</td>
        </tr>
    
        <tr>
            <th>Duration</th>
            <td>00:00:28 (HH:MM:SS)</td>
        </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 215-216

You can then save the coregistered object.

.. GENERATED FROM PYTHON SOURCE LINES 216-219

.. code-block:: default


    mne_nirs.io.write_raw_snirf(raw, "raw_coregistered_to_fsaverage.snirf")








.. GENERATED FROM PYTHON SOURCE LINES 220-222

You can then load this data and use it immediately as it has
already been coregistered to fsaverage.

.. GENERATED FROM PYTHON SOURCE LINES 222-230

.. code-block:: default


    raw_w_coreg = mne.io.read_raw_snirf("raw_coregistered_to_fsaverage.snirf")

    # Now you can simply use `trans = "fsaverage"`.
    brain = mne.viz.Brain('fsaverage', subjects_dir=subjects_dir, background='w', cortex='0.5', alpha=0.3)
    brain.add_sensors(raw_w_coreg.info, trans="fsaverage", fnirs=['sources', 'detectors'])





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_007.png
   :alt: plot 06 gowerlabs
   :srcset: /auto_examples/general/images/sphx_glr_plot_06_gowerlabs_007.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading /home/circleci/project/examples/general/raw_coregistered_to_fsaverage.snirf
    Channel types:: fnirs_cw_amplitude: 216




.. GENERATED FROM PYTHON SOURCE LINES 231-237

Next Steps
----------
From here you can use your favorite analysis technique such as
:ref:`tut-fnirs-processing` or :ref:`tut-fnirs-hrf`.

.. note:: HD-DOT specific tutorials are currently under development.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  39.240 seconds)

**Estimated memory usage:**  215 MB


.. _sphx_glr_download_auto_examples_general_plot_06_gowerlabs.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/mne-tools/mne-nirs/gh-pages?filepath=dev/notebooks/auto_examples/general/plot_06_gowerlabs.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_06_gowerlabs.py <plot_06_gowerlabs.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_06_gowerlabs.ipynb <plot_06_gowerlabs.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
