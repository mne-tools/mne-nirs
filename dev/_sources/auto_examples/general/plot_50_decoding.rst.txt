
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/general/plot_50_decoding.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_general_plot_50_decoding.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_general_plot_50_decoding.py:


.. _tut-fnirs-decoding:

Decoding Analysis
=================

This is an example of a decoding analysis performed on
functional near-infrared spectroscopy (fNIRS) data using
MNE-Python, scikit-learn, and MNE-NIRS.

Detailed information about decoding of neural signals can be found
in the MNE-Python documentation. For example see
`Decoding (MVPA) <https://mne.tools/stable/auto_examples/decoding/decoding_csp_eeg.html>`_,
:ref:`Linear classifier on sensor data  <mne:ex-linear-patterns>`,
:ref:`Decoding source space data <mne:ex-dec-st-source>`.
This example will use the techniques covered in the MNE-Python tutorials,
but applied specifically to fNIRS data.

This script is an example of analysis performed in the manuscript
Luke et. al. (2021)
:footcite:`Luke2021.11.19.469225`.

.. note::

   This tutorial uses data stored using
   `the BIDS format <https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/11-near-infrared-spectroscopy.html>`_
   :footcite:p:`luke2023bids`.

   MNE-Python allows you to process fNIRS data that is not in BIDS format.
   Simply modify the ``read_raw_`` function to match your data type.
   See :ref:`data importing tutorial <tut-importing-fnirs-data>` to learn how
   to use your data with MNE-Python.


.. contents:: Page contents
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 39-68

.. code-block:: Python


    # Authors: Robert Luke <mail@robertluke.net>
    #          Alexandre Gramfort <alexandre.gramfort@inria.fr>
    # License: BSD (3-clause)


    # Import common libraries
    import os
    import contextlib
    import numpy as np

    # Import sklearn processing
    from sklearn.pipeline import make_pipeline
    from sklearn.linear_model import LogisticRegression

    # Import MNE-Python processing
    from mne.preprocessing.nirs import optical_density, beer_lambert_law
    from mne import Epochs, events_from_annotations
    from mne.decoding import (Scaler,
                              cross_val_multiscore,
                              Vectorizer)

    # Import MNE-NIRS processing
    from mne_nirs.datasets.fnirs_motor_group import data_path

    # Import MNE-BIDS processing
    from mne_bids import BIDSPath, read_raw_bids, get_entity_vals









.. GENERATED FROM PYTHON SOURCE LINES 69-83

Set up directories
------------------
.. sidebar:: Requires MNE-BIDS fNIRS branch

   This section of code requires the MNE-BIDS fNIRS branch.
   See instructions at the top of the page on how to install.
   Alternatively, if your data is not in BIDS format,
   skip to the next section.

First we will define where the raw data is stored. We will analyse a
BIDS dataset. This ensures we have all the metadata we require
without manually specifying the trigger names etc.
We first define where the root directory of our dataset is.
In this example we use the example dataset ``audio_or_visual_speech``.

.. GENERATED FROM PYTHON SOURCE LINES 83-90

.. code-block:: Python


    root = data_path()
    dataset = BIDSPath(root=root, suffix="nirs", extension=".snirf",
                       task="tapping", datatype="nirs")
    subjects = get_entity_vals(root, 'subject')









.. GENERATED FROM PYTHON SOURCE LINES 91-100

Define individual analysis
--------------------------

More details on the epoching analysis can be found
at :ref:`Waveform individual analysis <tut-fnirs-processing>`.
A minimal processing pipeline is demonstrated here, as the focus
of this tutorial is to demonstrate the decoding pipeline.
In this example only the epochs for the two conditions we wish to decode
between are retained.

.. GENERATED FROM PYTHON SOURCE LINES 100-123

.. code-block:: Python



    def epoch_preprocessing(bids_path):

        with open(os.devnull, "w") as f, contextlib.redirect_stdout(f):
            raw_intensity = read_raw_bids(bids_path=bids_path).load_data()

        raw_od = optical_density(raw_intensity)
        raw_od.resample(1.5)
        raw_haemo = beer_lambert_law(raw_od, ppf=6)
        raw_haemo = raw_haemo.filter(None, 0.6, h_trans_bandwidth=0.05,
                                     l_trans_bandwidth=0.01, verbose=False)

        events, event_dict = events_from_annotations(raw_haemo, verbose=False)
        epochs = Epochs(raw_haemo, events, event_id=event_dict, tmin=-5, tmax=30,
                        reject=dict(hbo=100e-6), reject_by_annotation=True,
                        proj=True, baseline=(None, 0), detrend=1,
                        preload=True, verbose=False)

        epochs = epochs[["Tapping/Right", "Tapping/Left"]]
        return raw_haemo, epochs









.. GENERATED FROM PYTHON SOURCE LINES 124-136

Run analysis on all participants
--------------------------------

Next we loop through each measurement and decode between the control and
audio condition.
Here we compute a single spatio-temporal metric approach that simultaneously
uses all channels and time points to estimate the experimental condition.
The data is scaled for each channel by the mean and standard deviation
from all time points and epochs, after which they were vectorized to
comply with the scikit-learn data structure, and a logistic regression
classifier was applied using the liblinear solver.
This approach classifies the data within, rather than across, subjects.

.. GENERATED FROM PYTHON SOURCE LINES 136-164

.. code-block:: Python



    for chroma in ['hbo', 'hbr']:

        st_scores = []
        for sub in subjects:

            bids_path = dataset.update(subject=sub)
            raw_haemo, epochs = epoch_preprocessing(bids_path)

            epochs.pick(chroma)

            X = epochs.get_data()
            y = epochs.events[:, 2]

            clf = make_pipeline(Scaler(epochs.info),
                                Vectorizer(),
                                LogisticRegression(solver='liblinear'))

            scores = 100 * cross_val_multiscore(clf, X, y,
                                                cv=5, n_jobs=1, scoring='roc_auc')

            st_scores.append(np.mean(scores, axis=0))

        print(f"Average spatio-temporal ROC-AUC performance ({chroma}) = "
              f"{np.round(np.mean(st_scores))} % ({np.round(np.std(st_scores))})")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Average spatio-temporal ROC-AUC performance (hbo) = 89.0 % (10.0)
    Average spatio-temporal ROC-AUC performance (hbr) = 85.0 % (14.0)




.. GENERATED FROM PYTHON SOURCE LINES 165-173

Conclusion
----------

Data were epoched then decoding was performed on the hbo signal and the hbr
signal. The HbO signal decodes the conditions with 6% greater accuracy
than the HbR signal. For further discussion about the efficacy of fNIRS
signals in decoding experimental condition see Luke et. al. (2021)
:footcite:`Luke2021.11.19.469225`.

.. GENERATED FROM PYTHON SOURCE LINES 176-180

Bibliography
-----------------------------------------------

.. footbibliography::


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 5.102 seconds)

**Estimated memory usage:**  9 MB


.. _sphx_glr_download_auto_examples_general_plot_50_decoding.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/mne-tools/mne-nirs/gh-pages?filepath=stable/notebooks/auto_examples/general/plot_50_decoding.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_50_decoding.ipynb <plot_50_decoding.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_50_decoding.py <plot_50_decoding.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
