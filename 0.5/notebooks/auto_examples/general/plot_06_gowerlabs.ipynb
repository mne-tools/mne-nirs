{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Read Gowerlabs LUMO data\n\n[LUMO](https://www.gowerlabs.co.uk/lumo)_ is a modular, wearable, \nhigh-density diffuse optical tomography (HD-DOT) system produced by\n[Gowerlabs](https://www.gowerlabs.co.uk)_. This tutorial demonstrates\nhow to load data from LUMO, and how to utilise 3D digitisation\ninformation collected with the HD-DOT measurement.\n\nTo analyse LUMO data using MNE-NIRS, use the [lumomat](https://github.com/Gowerlabs/lumomat)_\npackage to convert the native data to the SNIRF format.\n\nHD-DOT data is often collected with individual registration of the sensor\npositions. In this tutorial we demonstrate how to load HD-DOT data from a\nLUMO device, co-register the channels to a head, and visualise the resulting\nchannel space.\n\nThis tutorial uses the 3D graphical functionality provided by MNE-Python,\nto ensure you have all the required packages installed we recommend using the\n[official MNE installers.](https://mne.tools/stable/install/index.html)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 6\n\n# Authors: Robert Luke <code@robertluke.net>\n#\n# License: BSD (3-clause)\n\nimport os.path as op\nimport mne\nfrom mne.datasets.testing import data_path\n\nfrom mne.viz import set_3d_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Gowerlabs Example File\nFirst, we must instruct the software where the file we wish to import\nresides. In this example we will use a small test file that is\nincluded in the MNE testing data set. To load your own data, replace\nthe path stored in the `fname` variable by\nrunning `fname = /path/to/data.snirf`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The provided sample file includes only a small number of LUMO\n          tiles, and thus channels.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne_nirs.io\n\ntesting_path = data_path(download=True)\nfname = op.join(testing_path, 'SNIRF', 'GowerLabs', 'lumomat-1-1-0.snirf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can view the path to the data by calling the variable `fname`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fname"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the file we call the function :func:`mne:mne.io.read_raw_snirf`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw = mne.io.read_raw_snirf(fname, preload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can look at the file metadata by calling the variable `raw`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise Data\nNext, we visually inspect the data to get an overview of the data quality\nand signal annotations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.plot(duration=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe valid data in each channel, and note that the file includes a\nnumber of event annotations.\nAnnotations are a flexible tool to represent events in your experiment. \nThey can also be used to annotate other useful information such as bad\nsegments of data, participant movements, etc. We can inspect the\nannotations to ensure they match what we expect from our experiment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The implementation of annotations varies between manufacturers. Rather\nthan recording the onset and duration of a stimulus condition, LUMO records\ndiscrete event markers which have a nominal one second duration. Each\nmarker can consist of an arbitrary character or string. In this sample, \nthere were six `A` annotations, one `Cat` annotation, and two `Dog` \nannotations. We can view the specific data for each annotation by converting\nthe annotations to a dataframe.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.annotations.to_data_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Optode Positions in 3D Space\nThe position of optodes in 3D space is recorded and stored in the SNIRF file.\nThese positions are stored in head coordinate frame,\nfor a detailed overview of coordinate frames and how they are handled in MNE\nsee `mne:tut-source-alignment`.\n\nWithin the SNIRF file, the position of each optode is stored,\nalong with scalp landmarks (\u201cfiducials\u201d).\nThese positions are in an arbitrary space, and must be aligned to a scan of\nthe participants, or a generic head.\n\nFor this data, we do not have a MRI scan of the participants head.\nInstead, we will align the positions to a generic head created from\na collection of 40 MRI scans of real brains called\n[fsaverage](https://mne.tools/stable/auto_tutorials/forward/10_background_freesurfer.html#fsaverage)_.\n\nFirst, lets just look at the sensors in arbitrary space.\nBelow we see that there are three LUMO tiles, each with three sources\nand four detectors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subjects_dir = op.join(mne.datasets.sample.data_path(), 'subjects')\nmne.datasets.fetch_fsaverage(subjects_dir=subjects_dir)\n\nbrain = mne.viz.Brain('fsaverage', subjects_dir=subjects_dir, alpha=0.0, cortex='low_contrast', background=\"w\")\nbrain.add_sensors(raw.info, trans='fsaverage', fnirs=[\"sources\", \"detectors\"])\nbrain.show_view(azimuth=130, elevation=80, distance=700)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coregister Optodes to Template Head\nThe optode locations displayed above are floating in free space\nand need to be aligned to our chosen head.\nFirst, lets just look at the `fsaverage` head that we will use.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial we use an automated code based approach\n          to coregistration. You can also use the MNE-Python\n          coregistration GUI :func:`mne:mne.gui.coregistration`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_kwargs = dict(subjects_dir=subjects_dir,\n                   surfaces=\"brain\", dig=True, eeg=[],\n                   fnirs=['sources', 'detectors'], show_axes=True,\n                   coord_frame='head', mri_fiducials=True)\n\nfig = mne.viz.plot_alignment(trans=\"fsaverage\", subject=\"fsaverage\", **plot_kwargs)\nset_3d_view(figure=fig, azimuth=90, elevation=0, distance=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what a head model will look like. If you have an MRI from\nthe participant you can use freesurfer to generate the required files.\nFor further details on generating freesurfer reconstructions see\n`mne:tut-freesurfer-reconstruction`.\n\nIn the figure above you can see the brain in grey. You can also\nsee the MRI fiducial positions marked with diamonds.\nThe nasion fiducial is marked in green, the left and right\npreauricular points (LPA and RPA) in red and blue respectively.\n\nNext, we simultaneously plot the `fsaverage` head, and the\ndata we wish to align to this head. This process is called\ncoregistration and is described in several MNE-Python tutorials\nincluding `mne:tut-auto-coreg`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = mne.viz.plot_alignment(raw.info, trans=\"fsaverage\", subject=\"fsaverage\", **plot_kwargs)\nset_3d_view(figure=fig, azimuth=90, elevation=0, distance=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the figure above we can see the Gowerlabs optode positions\nand the participants digitised fiducials represented by circles.\nThe participant digitised fiducials are a large distance from MRI fiducials\non the head. To coregister the optodes to the head we will perform\na rotation and translation of the optode frame to minimise the\ndistance between the fiducials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coreg = mne.coreg.Coregistration(raw.info, \"fsaverage\", subjects_dir, fiducials=\"estimated\")\ncoreg.fit_fiducials(lpa_weight=1., nasion_weight=1., rpa_weight=1.)\n\nfig = mne.viz.plot_alignment(raw.info, trans=coreg.trans, subject=\"fsaverage\", **plot_kwargs)\nset_3d_view(figure=fig, azimuth=90, elevation=0, distance=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see in the figure above that after the `fit_fiducials` method was called,\nthe optodes are well aligned to the brain, and the distance between diamonds (MRI)\nand circles (subject) fiducials is minimised. There is still some distance\nbetween the fiducial pairs, this is because we used a generic head rather than\nan individualised MRI scan. You can also :func:`mne:mne.scale_mri` to scale\nthe generic MRI head.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "brain = mne.viz.Brain('fsaverage', subjects_dir=subjects_dir, background='w', cortex='0.5', alpha=0.3)\nbrain.add_sensors(raw.info, trans=coreg.trans, fnirs=['sources', 'detectors'])\nbrain.show_view(azimuth=90, elevation=90, distance=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Transformation to Raw Object\nYou may wish to apply the coregistration transformation to the raw\nobject. This can be useful if you want to save the file back to disk\nand not coregister again when rereading the file. Or for simpler\ninterface using the `fsaverage` head.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mtg = raw.get_montage()\nmtg.apply_trans(coreg.trans)\nraw.set_montage(mtg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can then save the coregistered object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mne_nirs.io.write_raw_snirf(raw, \"raw_coregistered_to_fsaverage.snirf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can then load this data and use it immediately as it has\nalready been coregistered to fsaverage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_w_coreg = mne.io.read_raw_snirf(\"raw_coregistered_to_fsaverage.snirf\")\n\n# Now you can simply use `trans = \"fsaverage\"`.\nbrain = mne.viz.Brain('fsaverage', subjects_dir=subjects_dir, background='w', cortex='0.5', alpha=0.3)\nbrain.add_sensors(raw_w_coreg.info, trans=\"fsaverage\", fnirs=['sources', 'detectors'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\nFrom here you can use your favorite analysis technique such as\n`tut-fnirs-processing` or `tut-fnirs-hrf`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>HD-DOT specific tutorials are currently under development.</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}