{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Group Level Waveform Analysis\n\nThis is an example of a group-level waveform-based\nfunctional near-infrared spectroscopy (fNIRS)\nanalysis in MNE-NIRS.\n\n.. sidebar:: Relevant literature\n\n   Luke, Robert, et al.\n   \"Analysis methods for measuring passive auditory fNIRS responses generated\n   by a block-design paradigm.\" Neurophotonics 8.2 (2021):\n   [025008](https://www.spiedigitallibrary.org/journals/neurophotonics/volume-8/issue-2/025008/Analysis-methods-for-measuring-passive-auditory-fNIRS-responses-generated-by/10.1117/1.NPh.8.2.025008.short).\n\n   Gorgolewski, Krzysztof J., et al.\n   \"The brain imaging data structure, a format for organizing and describing\n   outputs of neuroimaging experiments.\" Scientific data 3.1 (2016): 1-9.\n\n   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).\n   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.\n\nIndividual-level analysis of this data is described in the\n`MNE-NIRS fNIRS waveform tutorial <tut-fnirs-processing>`\nand the\n`MNE-NIRS fNIRS GLM tutorial <tut-fnirs-hrf>`.\nAs such, this example will skim over the individual-level details\nand focus on the group-level aspects of analysis.\nHere we describe how to process multiple measurements\nand how to summarise group-level effects either by\nvisual presentation of the results or with summary statistics.\n\nThe data used in this example is available\n[at this location](https://github.com/rob-luke/BIDS-NIRS-Tapping).\nThe data was collected from a finger tapping experiment\nthat is briefly described below.\nThe dataset contains 5 participants.\nThe example dataset is in\n[BIDS](https://bids.neuroimaging.io)\nformat and therefore already contains\ninformation about triggers, condition names, etc.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial uses data in the BIDS format.\n   The BIDS specification for NIRS data is still under development. See:\n   [fNIRS BIDS proposal](https://github.com/bids-standard/bids-specification/pull/802).\n   As such, to run this tutorial you must use the MNE-BIDS 0.10 or later.\n\n   MNE-Python allows you to process fNIRS data that is not in BIDS format too.\n   Simply modify the ``read_raw_`` function to match your data type.\n   See `data importing tutorial <tut-importing-fnirs-data>` to learn how\n   to use your data with MNE-Python.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Optodes were placed over the motor cortex using the standard NIRx motor\n   montage with 8 short channels added (see their web page for details).\n   To view the sensor locations run\n   `raw_intensity.plot_sensors()`.\n   A 5 sec sound was presented in either the left or right ear to indicate\n   which hand the participant should tap.\n   Participants tapped their thumb to their fingers for the entire 5 sec.\n   Conditions were presented in a random order with a randomised inter-\n   stimulus interval.</p></div>\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2\n\n# Authors: Robert Luke <mail@robertluke.net>\n#\n# License: BSD (3-clause)\n\n# Import common libraries\nimport pandas as pd\nfrom itertools import compress\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom pprint import pprint\n\n# Import MNE processing\nfrom mne.viz import plot_compare_evokeds\nfrom mne import Epochs, events_from_annotations, set_log_level\n\n# Import MNE-NIRS processing\nfrom mne_nirs.channels import get_long_channels\nfrom mne_nirs.channels import picks_pair_to_idx\nfrom mne_nirs.datasets import fnirs_motor_group\nfrom mne.preprocessing.nirs import beer_lambert_law, optical_density,\\\n    temporal_derivative_distribution_repair, scalp_coupling_index\nfrom mne_nirs.signal_enhancement import enhance_negative_correlation\n\n# Import MNE-BIDS processing\nfrom mne_bids import BIDSPath, read_raw_bids\n\n# Import StatsModels\nimport statsmodels.formula.api as smf\n\n# Import Plotting Library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set general parameters\nset_log_level(\"WARNING\")  # Don't show info, as it is repetitive for many subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define individual analysis\n\n.. sidebar:: Individual analysis procedures\n\n   `Waveform individual analysis <tut-fnirs-processing>`\n\n   `GLM individual analysis <tut-fnirs-hrf>`\n\nFirst, we define the analysis that will be applied to each participant file.\nThis is the same waveform analysis as described in the\n`individual waveform tutorial <tut-fnirs-processing>`\nand `artifact correction tutorial <ex-fnirs-artifacts>`.\nAs such, this example will skim over the individual-level details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def individual_analysis(bids_path):\n\n    # Read data with annotations in BIDS format\n    raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)\n    raw_intensity = get_long_channels(raw_intensity, min_dist=0.01)\n\n    # Convert signal to optical density and determine bad channels\n    raw_od = optical_density(raw_intensity)\n    sci = scalp_coupling_index(raw_od, h_freq=1.35, h_trans_bandwidth=0.1)\n    raw_od.info[\"bads\"] = list(compress(raw_od.ch_names, sci < 0.5))\n    raw_od.interpolate_bads()\n\n    # Downsample and apply signal cleaning techniques\n    raw_od.resample(0.8)\n    raw_od = temporal_derivative_distribution_repair(raw_od)\n\n    # Convert to haemoglobin and filter\n    raw_haemo = beer_lambert_law(raw_od, ppf=0.1)\n    raw_haemo = raw_haemo.filter(0.02, 0.3,\n                                 h_trans_bandwidth=0.1, l_trans_bandwidth=0.01,\n                                 verbose=False)\n\n    # Apply further data cleaning techniques and extract epochs\n    raw_haemo = enhance_negative_correlation(raw_haemo)\n    # Extract events but ignore those with\n    # the word Ends (i.e. drop ExperimentEnds events)\n    events, event_dict = events_from_annotations(raw_haemo, verbose=False,\n                                                 regexp='^(?![Ends]).*$')\n    epochs = Epochs(raw_haemo, events, event_id=event_dict, tmin=-5, tmax=20,\n                    reject=dict(hbo=200e-6), reject_by_annotation=True,\n                    proj=True, baseline=(None, 0), detrend=0,\n                    preload=True, verbose=False)\n\n    return raw_haemo, epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run analysis on all data\n\nNext, we loop through the five participants' measurements and run the\nindividual analysis on each. For each individual, the function\nreturns the raw data and an epoch structure. The epoch structure is\nthen averaged to obtain an evoked response from each participant.\nThe individual-evoked data is stored in a\ndictionary (`all_evokeds`) for each condition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_evokeds = defaultdict(list)\n\nfor sub in range(1, 6):  # Loop from first to fifth subject\n\n    # Create path to file based on experiment info\n    bids_path = BIDSPath(subject=\"%02d\" % sub, task=\"tapping\", datatype=\"nirs\",\n                         root=fnirs_motor_group.data_path(), suffix=\"nirs\",\n                         extension=\".snirf\")\n\n    # Analyse data and return both ROI and channel results\n    raw_haemo, epochs = individual_analysis(bids_path)\n\n    # Save individual-evoked participant data along with others in all_evokeds\n    for cidx, condition in enumerate(epochs.event_id):\n        all_evokeds[condition].append(epochs[condition].average())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The end result is a dictionary that is indexed per condition,\nwith each item in the dictionary being a list of evoked responses.\nSee below that for each condition we have obtained a MNE evoked type\nthat is generated from the average of all 30 trials and epoched from -5 to\n20 seconds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint(all_evokeds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View average waveform\n\nNext, a grand average epoch waveform is generated for each condition.\nThis is generated using all of the standard (long) fNIRS channels,\nas illustrated by the head inset in the top right corner of the figure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Specify the figure size and limits per chromophore\nfig, axes = plt.subplots(nrows=1, ncols=len(all_evokeds), figsize=(17, 5))\nlims = dict(hbo=[-5, 12], hbr=[-5, 12])\n\nfor (pick, color) in zip(['hbo', 'hbr'], ['r', 'b']):\n    for idx, evoked in enumerate(all_evokeds):\n        plot_compare_evokeds({evoked: all_evokeds[evoked]}, combine='mean',\n                             picks=pick, axes=axes[idx], show=False,\n                             colors=[color], legend=False, ylim=lims, ci=0.95,\n                             show_sensors=idx == 2)\n        axes[idx].set_title('{}'.format(evoked))\naxes[0].legend([\"Oxyhaemoglobin\", \"Deoxyhaemoglobin\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this figure we observe that the response to the tapping condition\nwith the right hand appears larger than when no tapping occurred in the\ncontrol condition (similar for when tapping occurred with the left hand).\nWe test if this is the case in the analysis below.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate regions of interest\n.. sidebar:: Relevant literature\n\n   Zimeo Morais, G.A., Balardin, J.B. & Sato, J.R.\n   fNIRS Optodes\u2019 Location Decider (fOLD): a toolbox for probe arrangement\n   guided by brain regions-of-interest. Sci Rep 8, 3341 (2018).\n\n   Shader and Luke et al. \"The use of broad vs restricted regions of\n   interest in functional near-infrared spectroscopy for measuring cortical\n   activation to auditory-only and visual-only speech.\"\n   Hearing Research (2021): [108256](https://www.sciencedirect.com/science/article/pii/S0378595521000903).\n\nHere we specify two regions of interest (ROIs) by listing the source-detector\npairs of interest and then determining which channels these correspond to\nwithin the raw data structure. The channel indices are stored in a\ndictionary for access below.\nThe fOLD toolbox can be used to assist in the design of ROIs.\nAnd consideration should be paid to ensure optimal size ROIs are selected.\n\nIn this example, two ROIs are generated. One for the left motor cortex\nand one for the right motor cortex. These are called `Left_Hemisphere` and\n`Right_Hemisphere` and are stored in the `rois` dictionary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Specify channel pairs for each ROI\nleft = [[4, 3], [1, 3], [3, 3], [1, 2], [2, 3], [1, 1]]\nright = [[8, 7], [5, 7], [7, 7], [5, 6], [6, 7], [5, 5]]\n\n# Then generate the correct indices for each pair and store in dictionary\nrois = dict(Left_Hemisphere=picks_pair_to_idx(raw_haemo, left),\n            Right_Hemisphere=picks_pair_to_idx(raw_haemo, right))\n\npprint(rois)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create average waveform per ROI\n\nNext, an average waveform is generated per condition per region of interest.\nThis allows the researcher to view the responses elicited in different\nregions of the brain for each condition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Specify the figure size and limits per chromophore.\nfig, axes = plt.subplots(nrows=len(rois), ncols=len(all_evokeds),\n                         figsize=(15, 6))\nlims = dict(hbo=[-8, 16], hbr=[-8, 16])\n\nfor (pick, color) in zip(['hbo', 'hbr'], ['r', 'b']):\n    for ridx, roi in enumerate(rois):\n        for cidx, evoked in enumerate(all_evokeds):\n            if pick == 'hbr':\n                picks = rois[roi][1::2]  # Select only the hbr channels\n            else:\n                picks = rois[roi][0::2]  # Select only the hbo channels\n\n            plot_compare_evokeds({evoked: all_evokeds[evoked]}, combine='mean',\n                                 picks=picks, axes=axes[ridx, cidx],\n                                 show=False, colors=[color], legend=False,\n                                 ylim=lims, ci=0.95, show_sensors=cidx == 2)\n            axes[ridx, cidx].set_title(\"\")\n        axes[0, cidx].set_title(f\"{evoked}\")\n        axes[ridx, 0].set_ylabel(f\"{roi}\\nChromophore (\u0394\u03bcMol)\")\naxes[0, 0].legend([\"Oxyhaemoglobin\", \"Deoxyhaemoglobin\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this figure we observe that the response to tapping appears\nlargest in the brain region that is contralateral to the hand\nthat is tapping. We test if this is the case in the analysis below.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract evoked amplitude\n\nThe waveforms above provide a qualitative overview of the data.\nIt is also useful to perform a quantitative analysis based on the relevant\nfeatures in the dataset.\nHere we extract the average value of the waveform between\n5 and 7 seconds for each subject, condition, region of interest, and\nchromophore. The data is then stored in a dataframe, which is saved\nto a csv file for easy analysis in any statistical analysis software.\nWe also demonstrate two example analyses on these values below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['ID', 'ROI', 'Chroma', 'Condition', 'Value'])\n\nfor idx, evoked in enumerate(all_evokeds):\n    subj_id = 0\n    for subj_data in all_evokeds[evoked]:\n        subj_id += 1\n        for roi in rois:\n            for chroma in [\"hbo\", \"hbr\"]:\n                data = deepcopy(subj_data).pick(picks=rois[roi]).pick(chroma)\n                value = data.crop(tmin=5.0, tmax=7.0).data.mean() * 1.0e6\n\n                # Append metadata and extracted feature to dataframe\n                this_df = pd.DataFrame(\n                    {'ID': subj_id, 'ROI': roi, 'Chroma': chroma,\n                     'Condition': evoked, 'Value': value}, index=[0])\n                df = pd.concat([df, this_df], ignore_index=True)\ndf.reset_index(inplace=True, drop=True)\ndf['Value'] = pd.to_numeric(df['Value'])  # some Pandas have this as object\n\n# You can export the dataframe for analysis in your favorite stats program\ndf.to_csv(\"stats-export.csv\")\n\n# Print out the first entries in the dataframe\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View individual results\n\nThis figure simply summarises the information in the dataframe created above.\nWe observe that the values extracted from the waveform for the control\ncondition generally sit around 0. Whereas the tapping conditions have\nlarger values. There is quite some variation in the values for the tapping\nconditions, which is typical of a group-level result. Many factors affect the\nresponse amplitude in an fNIRS experiment, including skin thickness and\nskull thickness, both of which vary across the head and across participants.\nFor this reason, fNIRS is most appropriate for detecting changes within a\nsingle ROI between conditions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sns.catplot(x=\"Condition\", y=\"Value\", hue=\"ID\", data=df.query(\"Chroma == 'hbo'\"), ci=None, palette=\"muted\", height=4, s=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research question 1: Comparison of conditions\n\nIn this example question we ask: is the HbO response in the\nleft ROI to tapping with the right hand larger\nthan the response when not tapping (control)?\nFor this token example we subset the dataframe and then apply the mixed\neffect model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_data = df.query(\"Condition in ['Control', 'Tapping/Right']\")\ninput_data = input_data.query(\"Chroma in ['hbo']\")\ninput_data = input_data.query(\"ROI in ['Left_Hemisphere']\")\n\nroi_model = smf.mixedlm(\"Value ~ Condition\", input_data,\n                        groups=input_data[\"ID\"]).fit()\nroi_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model indicates that for the oxyhaemoglobin (HbO) data in the left\nROI, that the tapping condition with the right hand evokes\na 9.0 \u03bcMol larger response than the control condition.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research question 2: Are responses larger on the contralateral side to tapping?\n\nIn this example question we ask: when tapping, is the brain region on the\ncontralateral side of the brain to the tapping hand larger than the\nipsilateral side?\n\nFirst, the ROI data in the dataframe is encoded as ipsi- and contralateral\nto the tapping hand. Then the data is subset to just examine the two tapping\nconditions and the model is applied.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Encode the ROIs as ipsi- or contralateral to the hand that is tapping.\ndf[\"Hemisphere\"] = \"Unknown\"\ndf.loc[(df[\"Condition\"] == \"Tapping/Right\") &\n       (df[\"ROI\"] == \"Right_Hemisphere\"), \"Hemisphere\"] = \"Ipsilateral\"\ndf.loc[(df[\"Condition\"] == \"Tapping/Right\") &\n       (df[\"ROI\"] == \"Left_Hemisphere\"), \"Hemisphere\"] = \"Contralateral\"\ndf.loc[(df[\"Condition\"] == \"Tapping/Left\") &\n       (df[\"ROI\"] == \"Left_Hemisphere\"), \"Hemisphere\"] = \"Ipsilateral\"\ndf.loc[(df[\"Condition\"] == \"Tapping/Left\") &\n       (df[\"ROI\"] == \"Right_Hemisphere\"), \"Hemisphere\"] = \"Contralateral\"\n\n# Subset the data for example model\ninput_data = df.query(\"Condition in ['Tapping/Right', 'Tapping/Left']\")\ninput_data = input_data.query(\"Chroma in ['hbo']\")\nassert len(input_data)\n\nroi_model = smf.mixedlm(\"Value ~ Hemisphere\", input_data,\n                        groups=input_data[\"ID\"]).fit()\nroi_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model indicates that for the oxyhaemoglobin (HbO) data, the ipsilateral\nresponses are 4.2 \u03bcMol smaller than those on the contralateral side to the\nhand that is tapping.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}