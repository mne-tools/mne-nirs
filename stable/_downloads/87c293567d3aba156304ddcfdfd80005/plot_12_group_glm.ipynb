{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Group Level GLM Analysis\n\nThis is an example of a group level GLM based\nfunctional near-infrared spectroscopy (fNIRS)\nanalysis in MNE-NIRS.\n\n.. sidebar:: Relevant literature\n\n   Luke, Robert, et al.\n   \"Analysis methods for measuring passive auditory fNIRS responses generated\n   by a block-design paradigm.\" Neurophotonics 8.2 (2021):\n   [025008](https://www.spiedigitallibrary.org/journals/neurophotonics/volume-8/issue-2/025008/Analysis-methods-for-measuring-passive-auditory-fNIRS-responses-generated-by/10.1117/1.NPh.8.2.025008.short).\n\n   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).\n   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.\n\n   Gorgolewski, Krzysztof J., et al.\n   \"The brain imaging data structure, a format for organizing and describing\n   outputs of neuroimaging experiments.\" Scientific data 3.1 (2016): 1-9.\n\nIndividual level analysis of this data is described in the\n`MNE fNIRS waveform tutorial <mne:tut-fnirs-processing>`\nand the\n`MNE-NIRS fNIRS GLM tutorial <tut-fnirs-hrf>`\nSo this example will skim over the individual level details\nand focus on the group level aspect of analysis.\nHere we describe how to process multiple measurements\nand summarise  group level effects both as summary statistics and visually.\n\nThe data used in this example is available\n[at this location](https://github.com/rob-luke/BIDS-NIRS-Tapping).\nIt is a finger tapping example and is briefly described below.\nThe dataset contains 5 participants.\nThe example dataset is in\n[BIDS](https://bids.neuroimaging.io)\nformat and therefore already contains\ninformation about triggers, condition names, etc.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial uses data stored using\n   [the BIDS format](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/11-near-infrared-spectroscopy.html)\n   :footcite:p:`luke2023bids`.\n\n   MNE-Python allows you to process fNIRS data that is not in BIDS format.\n   Simply modify the ``read_raw_`` function to match your data type.\n   See `data importing tutorial <tut-importing-fnirs-data>` to learn how\n   to use your data with MNE-Python.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Optodes were placed over the motor cortex using the standard NIRX motor\n   montage, but with 8 short channels added (see their web page for details).\n   To view the sensor locations run\n   `raw_intensity.plot_sensors()`.\n   A sound was presented to indicate which hand the participant should tap.\n   Participants tapped their thumb to their fingers for 5s.\n   Conditions were presented in a random order with a randomised inter\n   stimulus interval.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2\n\n# Authors: Robert Luke <mail@robertluke.net>\n#\n# License: BSD (3-clause)\n\n# Import common libraries\nimport matplotlib as mpl\n\n# Import Plotting Library\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Import StatsModels\nimport statsmodels.formula.api as smf\n\n# Import MNE processing\nfrom mne.preprocessing.nirs import beer_lambert_law, optical_density\n\n# Import MNE-BIDS processing\nfrom mne_bids import BIDSPath, get_entity_vals, read_raw_bids\n\nfrom mne_nirs.channels import get_long_channels, get_short_channels, picks_pair_to_idx\nfrom mne_nirs.datasets import fnirs_motor_group\nfrom mne_nirs.experimental_design import make_first_level_design_matrix\nfrom mne_nirs.io.fold import fold_channel_specificity\n\n# Import MNE-NIRS processing\nfrom mne_nirs.statistics import run_glm, statsmodels_to_results\nfrom mne_nirs.visualisation import plot_glm_group_topo, plot_glm_surface_projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up directories\n\nFirst we will define where the raw data is stored. We will analyse a\nBIDS dataset. This ensures we have all the metadata we require\nwithout manually specifying the trigger names etc.\nWe first define where the root directory of our dataset is.\nIn this example we use the example dataset ``fnirs_motor_group``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = fnirs_motor_group.data_path()\nprint(root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And as we are using MNE-BIDS we can create a BIDSPath object.\nThis class helps to handle all the path wrangling.\nWe inform the software that we are analysing nirs data that is saved in\nthe snirf format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = BIDSPath(\n    root=root, task=\"tapping\", datatype=\"nirs\", suffix=\"nirs\", extension=\".snirf\"\n)\n\nprint(dataset.directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example we can automatically query the subjects, tasks, and sessions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subjects = get_entity_vals(root, \"subject\")\nprint(subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define individual analysis\n\n.. sidebar:: Individual analysis procedures\n\n   `Waveform individual analysis <tut-fnirs-processing>`\n\n   `GLM individual analysis <tut-fnirs-hrf>`\n\nFirst we define the analysis that will be applied to each file.\nThis is a GLM analysis as described in the\n`individual GLM tutorial <tut-fnirs-hrf>`,\nso this example will skim over the individual level details.\n\nThe analysis extracts a response estimate for each channel,\neach region of interest, and computes a contrast between left and right\nfinger tapping.\nWe return the raw object and data frames for the computed results.\nInformation about channels, triggers and their meanings are stored in the\nBIDS structure and are automatically obtained when importing the data.\n\nHere we also resample to a 0.3 Hz sample rate just to speed up the example\nand use less memory, resampling to 0.6 Hz is a better choice for full\nanalyses.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The nilearn library does not allow backslash characters in the condition\n   name. So we must replace the backslash with an underscore to ensure the\n   GLM computation is successful. Hopefully future versions of MNE-NIRS will\n   automatically handle these characters, see https://github.com/mne-tools/mne-nirs/issues/420\n   for more information. In the meantime use the following code to replace the\n   illegal characters.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def individual_analysis(bids_path, ID):\n    raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)\n    # Delete annotation labeled 15, as these just signify the experiment start and end.\n    raw_intensity.annotations.delete(raw_intensity.annotations.description == \"15.0\")\n    # sanitize event names\n    raw_intensity.annotations.description[:] = [\n        d.replace(\"/\", \"_\") for d in raw_intensity.annotations.description\n    ]\n\n    # Convert signal to haemoglobin and resample\n    raw_od = optical_density(raw_intensity)\n    raw_haemo = beer_lambert_law(raw_od, ppf=0.1)\n    raw_haemo.resample(0.3)\n\n    # Cut out just the short channels for creating a GLM repressor\n    sht_chans = get_short_channels(raw_haemo)\n    raw_haemo = get_long_channels(raw_haemo)\n\n    # Create a design matrix\n    design_matrix = make_first_level_design_matrix(raw_haemo, stim_dur=5.0)\n\n    # Append short channels mean to design matrix\n    design_matrix[\"ShortHbO\"] = np.mean(\n        sht_chans.copy().pick(picks=\"hbo\").get_data(), axis=0\n    )\n    design_matrix[\"ShortHbR\"] = np.mean(\n        sht_chans.copy().pick(picks=\"hbr\").get_data(), axis=0\n    )\n\n    # Run GLM\n    glm_est = run_glm(raw_haemo, design_matrix)\n\n    # Define channels in each region of interest\n    # List the channel pairs manually\n    left = [[4, 3], [1, 3], [3, 3], [1, 2], [2, 3], [1, 1]]\n    right = [[8, 7], [5, 7], [7, 7], [5, 6], [6, 7], [5, 5]]\n    # Then generate the correct indices for each pair\n    groups = dict(\n        Left_Hemisphere=picks_pair_to_idx(raw_haemo, left, on_missing=\"ignore\"),\n        Right_Hemisphere=picks_pair_to_idx(raw_haemo, right, on_missing=\"ignore\"),\n    )\n\n    # Extract channel metrics\n    cha = glm_est.to_dataframe()\n\n    # Compute region of interest results from channel data\n    roi = glm_est.to_dataframe_region_of_interest(\n        groups, design_matrix.columns, demographic_info=True\n    )\n\n    # Define left vs right tapping contrast\n    contrast_matrix = np.eye(design_matrix.shape[1])\n    basic_conts = dict(\n        [(column, contrast_matrix[i]) for i, column in enumerate(design_matrix.columns)]\n    )\n    contrast_LvR = basic_conts[\"Tapping_Left\"] - basic_conts[\"Tapping_Right\"]\n\n    # Compute defined contrast\n    contrast = glm_est.compute_contrast(contrast_LvR)\n    con = contrast.to_dataframe()\n\n    # Add the participant ID to the dataframes\n    roi[\"ID\"] = cha[\"ID\"] = con[\"ID\"] = ID\n\n    # Convert to uM for nicer plotting below.\n    cha[\"theta\"] = [t * 1.0e6 for t in cha[\"theta\"]]\n    roi[\"theta\"] = [t * 1.0e6 for t in roi[\"theta\"]]\n    con[\"effect\"] = [t * 1.0e6 for t in con[\"effect\"]]\n\n    return raw_haemo, roi, cha, con"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run analysis on all participants\n\nNext we loop through the five measurements and run the individual analysis\non each. We append the individual results in to a large dataframe that\nwill contain the results from all measurements. We create a group dataframe\nfor the region of interest, channel level, and contrast results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_roi = pd.DataFrame()  # To store region of interest results\ndf_cha = pd.DataFrame()  # To store channel level results\ndf_con = pd.DataFrame()  # To store channel level contrast results\n\nfor sub in subjects:  # Loop from first to fifth subject\n    # Create path to file based on experiment info\n    bids_path = dataset.update(subject=sub)\n\n    # Analyse data and return both ROI and channel results\n    raw_haemo, roi, channel, con = individual_analysis(bids_path, sub)\n\n    # Append individual results to all participants\n    df_roi = pd.concat([df_roi, roi], ignore_index=True)\n    df_cha = pd.concat([df_cha, channel], ignore_index=True)\n    df_con = pd.concat([df_con, con], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise Individual results\n\nFirst we visualise the results from each individual to ensure the\ndata values look reasonable.\nHere we see that we have data from five participants, we plot just the HbO\nvalues and observe they are in the expect range.\nWe can already see that the control condition is always near zero,\nand that the responses look to be contralateral to the tapping hand.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grp_results = df_roi.query(\"Condition in ['Control', 'Tapping_Left', 'Tapping_Right']\")\ngrp_results = grp_results.query(\"Chroma in ['hbo']\")\n\nsns.catplot(\n    x=\"Condition\",\n    y=\"theta\",\n    col=\"ID\",\n    hue=\"ROI\",\n    data=grp_results,\n    col_wrap=5,\n    errorbar=None,\n    palette=\"muted\",\n    height=4,\n    s=10,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute group level results\n\n.. sidebar:: Relevant literature\n\n   For an introduction to mixed effects analysis see:\n   Winter, Bodo. \"A very basic tutorial for performing linear mixed effects\n   analyses.\" arXiv preprint arXiv:1308.5499 (2013).\n\n   For a summary of linear mixed models in python\n   and the relation to lmer see:\n   `statsmodels docs <statsmodels:mixedlmmod>`\n\n   For a summary of these models in the context of fNIRS see section 3.5 of:\n   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).\n   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.\n\nNext we use a linear mixed effects model to examine the\nrelation between conditions and our response estimate (theta).\nCombinations of 3 fixed effects will be evaluated, ROI (left vs right),\ncondition (control, tapping/left, tapping/right), and chromophore (HbO, HbR).\nWith a random effect of subject.\nAlternatively, you could export the group dataframe (`df_roi.to_csv()`) and\nanalyse in your favorite stats program.\n\nWe do not explore the modeling procedure in depth here as topics\nsuch model selection and examining residuals are beyond the scope of\nthis example (see relevant literature).\nAlternatively, we could use a robust linear\nmodel by using the code\n`roi_model = rlm('theta ~ -1 + ROI:Condition:Chroma', grp_results).fit()`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grp_results = df_roi.query(\"Condition in ['Control','Tapping_Left', 'Tapping_Right']\")\n\nroi_model = smf.mixedlm(\n    \"theta ~ -1 + ROI:Condition:Chroma\", grp_results, groups=grp_results[\"ID\"]\n).fit(method=\"nm\")\nroi_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second level analysis with covariates\n\n.. sidebar:: Relevant literature\n\n   For a detailed discussion about covariates in fNIRS analysis see\n   the seminar by Dr. Jessica Gemignani\n   ([youtube](https://www.youtube.com/watch?feature=emb_logo&v=3E28sT1JI14)).\n\nIt is simple to extend these models to include covariates.\nThis dataset is small, so including additional factors may not be\nappropriate. However, for instructional purpose, we will include a\ncovariate of gender. There are 3 females and 2 males in this dataset.\nAlso, for instructional purpose, we modify the model\nabove to only explore the difference between the two tapping conditions in\nthe hbo signal in the right hemisphere.\n\nFrom the model result we observe that hbo responses in the right hemisphere\nare smaller when the right hand was used (as expected for these\ncontralaterally dominant responses) and there is no significant\neffect of gender.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grp_results = df_roi.query(\"Condition in ['Tapping_Left', 'Tapping_Right']\")\ngrp_results = grp_results.query(\"Chroma in ['hbo']\")\ngrp_results = grp_results.query(\"ROI in ['Right_Hemisphere']\")\n\nroi_model = smf.mixedlm(\n    \"theta ~ Condition + Sex\", grp_results, groups=grp_results[\"ID\"]\n).fit(method=\"nm\")\nroi_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise group results\n\nNow we can summarise the output of the second level model.\nThis figure shows that the control condition has small responses that\nare not significantly different to zero for both HbO\nand HbR in both hemispheres.\nWhereas clear significant responses are show for the two tapping conditions.\nWe also observe the the tapping response is\nlarger in the contralateral hemisphere.\nFilled symbols represent HbO, unfilled symbols represent HbR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Regenerate the results from the original group model above\ngrp_results = df_roi.query(\"Condition in ['Control','Tapping_Left', 'Tapping_Right']\")\nroi_model = smf.mixedlm(\n    \"theta ~ -1 + ROI:Condition:Chroma\", grp_results, groups=grp_results[\"ID\"]\n).fit(method=\"nm\")\n\ndf = statsmodels_to_results(roi_model)\n\nsns.catplot(\n    x=\"Condition\",\n    y=\"Coef.\",\n    hue=\"ROI\",\n    data=df.query(\"Chroma == 'hbo'\"),\n    errorbar=None,\n    palette=\"muted\",\n    height=4,\n    s=10,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Group topographic visualisation\n\nWe can also view the topographic representation of the data\n(rather than the ROI summary above).\nHere we just plot the oxyhaemoglobin for the two tapping conditions.\nFirst we compute the mixed effects model for each channel (rather\nthan region of interest as above).\nThen we pass these results to the topomap function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(\n    nrows=2, ncols=2, figsize=(10, 10), gridspec_kw=dict(width_ratios=[1, 1])\n)\n\n# Cut down the dataframe just to the conditions we are interested in\nch_summary = df_cha.query(\"Condition in ['Tapping_Left', 'Tapping_Right']\")\nch_summary = ch_summary.query(\"Chroma in ['hbo']\")\n\n# Run group level model and convert to dataframe\nch_model = smf.mixedlm(\n    \"theta ~ -1 + ch_name:Chroma:Condition\", ch_summary, groups=ch_summary[\"ID\"]\n).fit(method=\"nm\")\nch_model_df = statsmodels_to_results(ch_model)\n\n# Plot the two conditions\nplot_glm_group_topo(\n    raw_haemo.copy().pick(picks=\"hbo\"),\n    ch_model_df.query(\"Condition in ['Tapping_Left']\"),\n    colorbar=False,\n    axes=axes[0, 0],\n    vlim=(0, 20),\n    cmap=mpl.cm.Oranges,\n)\n\nplot_glm_group_topo(\n    raw_haemo.copy().pick(picks=\"hbo\"),\n    ch_model_df.query(\"Condition in ['Tapping_Right']\"),\n    colorbar=True,\n    axes=axes[0, 1],\n    vlim=(0, 20),\n    cmap=mpl.cm.Oranges,\n)\n\n# Cut down the dataframe just to the conditions we are interested in\nch_summary = df_cha.query(\"Condition in ['Tapping_Left', 'Tapping_Right']\")\nch_summary = ch_summary.query(\"Chroma in ['hbr']\")\n\n# Run group level model and convert to dataframe\nch_model = smf.mixedlm(\n    \"theta ~ -1 + ch_name:Chroma:Condition\", ch_summary, groups=ch_summary[\"ID\"]\n).fit(method=\"nm\")\nch_model_df = statsmodels_to_results(ch_model)\n\n# Plot the two conditions\nplot_glm_group_topo(\n    raw_haemo.copy().pick(picks=\"hbr\"),\n    ch_model_df.query(\"Condition in ['Tapping_Left']\"),\n    colorbar=False,\n    axes=axes[1, 0],\n    vlim=(-10, 0),\n    cmap=mpl.cm.Blues_r,\n)\nplot_glm_group_topo(\n    raw_haemo.copy().pick(picks=\"hbr\"),\n    ch_model_df.query(\"Condition in ['Tapping_Right']\"),\n    colorbar=True,\n    axes=axes[1, 1],\n    vlim=(-10, 0),\n    cmap=mpl.cm.Blues_r,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contrasts\n\nFinally we can examine the difference between the left and right hand\ntapping conditions by viewing the contrast results\nin a topographic representation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\ncon_summary = df_con.query(\"Chroma in ['hbo']\")\n\n# Run group level model and convert to dataframe\ncon_model = smf.mixedlm(\n    \"effect ~ -1 + ch_name:Chroma\", con_summary, groups=con_summary[\"ID\"]\n).fit(method=\"nm\")\ncon_model_df = statsmodels_to_results(\n    con_model, order=raw_haemo.copy().pick(picks=\"hbo\").ch_names\n)\n\nplot_glm_group_topo(\n    raw_haemo.copy().pick(picks=\"hbo\"), con_model_df, colorbar=True, axes=axes\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or we can view only the left hemisphere for the contrast.\nAnd set all channels that dont have a significant response to zero.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_glm_group_topo(\n    raw_haemo.copy().pick(picks=\"hbo\").pick(picks=range(10)),\n    con_model_df,\n    colorbar=True,\n    threshold=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cortical Surface Projections\n\nThe topographic plots above can sometimes be difficult to interpret with\nrespect to the underlying cortical locations. It is also possible to present\nthe data by projecting the channel level GLM values to the nearest cortical\nsurface. This can make it easier to understand the spatial aspects of your\ndata. Note however, that this is not a complete forward model with photon\nmigration simulations.\nIn the figure below we project the group results from the two conditions\nto the cortical surface, and also present the contrast results in the same\nfashion.\nAs in the topo plots above you can see that the activity is predominately\ncontralateral to the side of finger tapping.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate brain figure from data\nclim = dict(kind=\"value\", pos_lims=(0, 8, 11))\nbrain = plot_glm_surface_projection(\n    raw_haemo.copy().pick(\"hbo\"),\n    con_model_df,\n    clim=clim,\n    view=\"dorsal\",\n    colorbar=True,\n    size=(800, 700),\n)\nbrain.add_text(0.05, 0.95, \"Left-Right\", \"title\", font_size=16, color=\"k\")\n\n# Run model code as above\nclim = dict(kind=\"value\", pos_lims=(0, 11.5, 17))\nfor idx, cond in enumerate([\"Tapping_Left\", \"Tapping_Right\"]):\n    # Run same model as explained in the sections above\n    ch_summary = df_cha.query(\"Condition in [@cond]\")\n    ch_summary = ch_summary.query(\"Chroma in ['hbo']\")\n    ch_model = smf.mixedlm(\n        \"theta ~ -1 + ch_name\", ch_summary, groups=ch_summary[\"ID\"]\n    ).fit(method=\"nm\")\n    model_df = statsmodels_to_results(\n        ch_model, order=raw_haemo.copy().pick(\"hbo\").ch_names\n    )\n\n    # Generate brain figure from data\n    brain = plot_glm_surface_projection(\n        raw_haemo.copy().pick(\"hbo\"),\n        model_df,\n        clim=clim,\n        view=\"dorsal\",\n        colorbar=True,\n        size=(800, 700),\n    )\n    brain.add_text(0.05, 0.95, cond, \"title\", font_size=16, color=\"k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of channel level results\n\nSometimes a reviewer wants a long table of results per channel.\nThis can be generated from the statistics dataframe.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ch_summary = df_cha.query(\"Condition in ['Tapping_Left', 'Tapping_Right']\")\nch_summary = ch_summary.query(\"Chroma in ['hbo']\")\n\n# Run group level model and convert to dataframe\nch_model = smf.mixedlm(\n    \"theta ~ -1 + ch_name:Chroma:Condition\", ch_summary, groups=ch_summary[\"ID\"]\n).fit(method=\"nm\")\n\n# Here we can use the order argument to ensure the channel name order\nch_model_df = statsmodels_to_results(\n    ch_model, order=raw_haemo.copy().pick(picks=\"hbo\").ch_names\n)\n# And make the table prettier\nch_model_df.reset_index(drop=True, inplace=True)\nch_model_df = ch_model_df.set_index([\"ch_name\", \"Condition\"])\nch_model_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Relating Responses to Brain Landmarks\n\n.. sidebar:: fOLD Toolbox\n\n   You should use the fOLD toolbox to pick your optode locations\n   when designing your experiment.\n   The tool is very intuitive and easy to use.\n   Be sure to cite the authors if you use their tool or data:\n\n   Morais, Guilherme Augusto Zimeo, Joana Bisol Balardin, and Jo\u00e3o Ricardo Sato.\n   \"fNIRS optodes\u2019 location decider (fOLD): a toolbox for probe arrangement guided by\n   brain regions-of-interest.\" Scientific reports 8.1 (2018): 1-11.\n\nIt can be useful to understand what brain structures\nthe measured response may have resulted from. Here we illustrate\nhow to report the brain structures/landmarks that the source\ndetector pair with the largest response was sensitive to.\n\nFirst we determine the channel with the largest response.\n\nNext, we query the fOLD dataset to determine the\nbrain landmarks that this channel is most sensitive to.\nMNE-NIRS does not distribute the fOLD toolbox or the data\nthat they provide. See the Notes section of\n:func:`mne_nirs.io.fold_channel_specificity` for more information.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "largest_response_channel = ch_model_df.loc[ch_model_df[\"Coef.\"].idxmax()]\nlargest_response_channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we use information from the fOLD toolbox to report the\nchannel specificity to different brain regions.\nFor licensing reasons, these files are not distributed with MNE-NIRS.\nTo set up your system to use the fOLD functions, see the Notes section of\n:func:`mne_nirs.io.fold_channel_specificity`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_channel = raw_haemo.copy().pick(largest_response_channel.name[0])\nfold_channel_specificity(raw_channel)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the channel with the largest response to tapping\nhad the greatest specificity to the Precentral Gyrus, which is\nthe site of the primary motor cortex. This is consistent\nwith the expectation for a finger tapping task.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nThis example has demonstrated how to perform a group level analysis\nusing a GLM approach.\nWe observed the responses were evoked primarily contralateral to the\nhand of tapping and most likely originate from the primary motor cortex.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}